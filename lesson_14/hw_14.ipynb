{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.  –î–æ–æ–±—É—á–∏—Ç—å –±–µ—Ä—Ç –Ω–∞ –∑–∞–¥–∞—á—É NER\n",
    "2.  –î–æ–æ–±—É—á–∏—Ç—å GPT –Ω–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏—é —Ç–µ–∫—Å—Ç–∞\n",
    "3. –î–æ–æ–±—É—á–∏—Ç—å T5 –Ω–∞ –∑–∞–¥–∞—á—É —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-28 22:13:07.186585: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-01-28 22:13:07.186621: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-01-28 22:13:07.188096: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-01-28 22:13:07.196880: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-01-28 22:13:08.468100: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import re\n",
    "import os\n",
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from razdel import tokenize\n",
    "from corus import load_rudrec\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from datasets import load_dataset, load_metric, Dataset, DatasetDict\n",
    "from collections import Counter, defaultdict\n",
    "from transformers import AutoTokenizer, T5ForConditionalGeneration, Trainer, TrainingArguments, DataCollatorForLanguageModeling, TextDataset, AutoModelForCausalLM, AutoModelForTokenClassification, DataCollatorForTokenClassification, pipeline\n",
    "from transformers.trainer import logger as noisy_logger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –î–æ–æ–±—É—á–µ–Ω–∏–µ BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path_r = r'/media/dmitriy/Disk/Downloads/ai_nlp_hw_data/hw_14/rudrec/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = r'cointegrated/rubert-tiny'\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "drugs = list(load_rudrec(os.path.join(data_path_r, r'rudrec_annotated.json')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DI 1401\n",
      "[('–ø—Ä–æ—Å—Ç—É–¥—ã', 64), ('–û–†–í–ò', 47), ('–ø—Ä–æ—Ñ–∏–ª–∞–∫—Ç–∏–∫–∏', 42)]\n",
      "Drugname 1043\n",
      "[('–í–∏—Ñ–µ—Ä–æ–Ω', 33), ('–ê–Ω–∞—Ñ–µ—Ä–æ–Ω', 25), ('–¶–∏–∫–ª–æ—Ñ–µ—Ä–æ–Ω', 24)]\n",
      "Drugform 836\n",
      "[('—Ç–∞–±–ª–µ—Ç–∫–∏', 154), ('—Ç–∞–±–ª–µ—Ç–æ–∫', 79), ('—Å–≤–µ—á–∏', 63)]\n",
      "ADR 720\n",
      "[('–∞–ª–ª–µ—Ä–≥–∏—è', 16), ('—Å–ª–∞–±–æ—Å—Ç—å', 13), ('–¥–∏–∞—Ä–µ—è', 12)]\n",
      "Drugclass 330\n",
      "[('–ø—Ä–æ—Ç–∏–≤–æ–≤–∏—Ä—É—Å–Ω—ã–π', 21), ('–ø—Ä–æ—Ç–∏–≤–æ–≤–∏—Ä—É—Å–Ω–æ–µ', 18), ('–ø—Ä–æ—Ç–∏–≤–æ–≤–∏—Ä—É—Å–Ω—ã—Ö', 13)]\n",
      "Finding 236\n",
      "[('–∞–ª–ª–µ—Ä–≥–∏–∏', 12), ('—Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä—ã', 6), ('—Å–æ–Ω–ª–∏–≤–æ—Å—Ç–∏', 5)]\n"
     ]
    }
   ],
   "source": [
    "type2text = defaultdict(Counter)\n",
    "ents = Counter()\n",
    "for item in drugs:\n",
    "    for e in item.entities:\n",
    "        ents[e.entity_type] += 1\n",
    "        type2text[e.entity_type][e.entity_text] += 1\n",
    "for k, v in ents.most_common():\n",
    "    print(k, v)\n",
    "    print(type2text[k].most_common(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_labels(item):\n",
    "    raw_toks = list(tokenize(item.text))\n",
    "    words = [tok.text for tok in raw_toks]\n",
    "    word_labels = ['O'] * len(raw_toks)\n",
    "    char2word = [None] * len(item.text)\n",
    "    for i, word in enumerate(raw_toks):\n",
    "        char2word[word.start:word.stop] = [i] * len(word.text)\n",
    "    for e in item.entities:\n",
    "        e_words = sorted({idx for idx in char2word[e.start:e.end] if idx is not None})\n",
    "        word_labels[e_words[0]] = 'B-' + e.entity_type\n",
    "        for idx in e_words[1:]:\n",
    "            word_labels[idx] = 'I-' + e.entity_type\n",
    "    return {'tokens': words, 'tags': word_labels}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_data = [extract_labels(item) for item in drugs]\n",
    "ner_train, ner_test = train_test_split(ner_data, test_size=0.1, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokens</th>\n",
       "      <th>tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>316</th>\n",
       "      <td>[–ü—Ä–µ–ø–∞—Ä–∞—Ç, \", –õ–∏–∫–æ–ø–∏–¥, \", –ø–æ–∫—É–ø–∞–ª–∞, –¥–ª—è, –ª–µ—á–µ–Ω–∏—è, –≥–µ—Ä–ø–µ—Å–∞, .]</td>\n",
       "      <td>[O, O, B-Drugname, O, O, O, O, B-DI, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>885</th>\n",
       "      <td>[–ü–µ—Ä–≤—ã–µ, —Å—É—Ç–∫–∏, .]</td>\n",
       "      <td>[O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>577</th>\n",
       "      <td>[–ï—Å–ª–∏, —Ä–∞–Ω—å—à–µ, –±—ã–ª–æ, –ª—É—á—à–µ, ,, —Ç–æ, —Å–µ–π—á–∞—Å, –∫–∞—á–µ—Å–≤–æ, –ø—Ä–µ–ø–∞—Ä–∞—Ç–∞, –æ—Å—Ç–∞–≤–ª—è–µ—Ç, –∂–µ–ª–∞—Ç—å, –ª—É—á—à–µ–≥–æ, .]</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                            tokens  \\\n",
       "316                                  [–ü—Ä–µ–ø–∞—Ä–∞—Ç, \", –õ–∏–∫–æ–ø–∏–¥, \", –ø–æ–∫—É–ø–∞–ª–∞, –¥–ª—è, –ª–µ—á–µ–Ω–∏—è, –≥–µ—Ä–ø–µ—Å–∞, .]   \n",
       "885                                                                             [–ü–µ—Ä–≤—ã–µ, —Å—É—Ç–∫–∏, .]   \n",
       "577  [–ï—Å–ª–∏, —Ä–∞–Ω—å—à–µ, –±—ã–ª–æ, –ª—É—á—à–µ, ,, —Ç–æ, —Å–µ–π—á–∞—Å, –∫–∞—á–µ—Å–≤–æ, –ø—Ä–µ–ø–∞—Ä–∞—Ç–∞, –æ—Å—Ç–∞–≤–ª—è–µ—Ç, –∂–µ–ª–∞—Ç—å, –ª—É—á—à–µ–≥–æ, .]   \n",
       "\n",
       "                                        tags  \n",
       "316  [O, O, B-Drugname, O, O, O, O, B-DI, O]  \n",
       "885                                [O, O, O]  \n",
       "577  [O, O, O, O, O, O, O, O, O, O, O, O, O]  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.options.display.max_colwidth = 300\n",
    "pd.DataFrame(ner_train).sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O',\n",
       " 'B-ADR',\n",
       " 'B-DI',\n",
       " 'B-Drugclass',\n",
       " 'B-Drugform',\n",
       " 'B-Drugname',\n",
       " 'B-Finding',\n",
       " 'I-ADR',\n",
       " 'I-DI',\n",
       " 'I-Drugclass',\n",
       " 'I-Drugform',\n",
       " 'I-Drugname',\n",
       " 'I-Finding']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_list = sorted({label for item in ner_train for label in item['tags']})\n",
    "if 'O' in label_list:\n",
    "    label_list.remove('O')\n",
    "    label_list = ['O'] + label_list\n",
    "label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['tokens', 'tags'],\n",
       "        num_rows: 4328\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['tokens', 'tags'],\n",
       "        num_rows: 481\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner_data = DatasetDict({\n",
    "    'train': Dataset.from_pandas(pd.DataFrame(ner_train)),\n",
    "    'test': Dataset.from_pandas(pd.DataFrame(ner_test))\n",
    "})\n",
    "ner_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(examples, label_all_tokens=True):\n",
    "    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True)\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples['tags']):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            # Special tokens have a word id that is None. We set the label to -100 so they are automatically\n",
    "            # ignored in the loss function.\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            # We set the label for the first token of each word.\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(label[word_idx])\n",
    "            # For the other tokens in a word, we set the label to either the current label or -100, depending on\n",
    "            # the label_all_tokens flag.\n",
    "            else:\n",
    "                label_ids.append(label[word_idx] if label_all_tokens else -100)\n",
    "            previous_word_idx = word_idx\n",
    "        label_ids = [label_list.index(idx) if isinstance(idx, str) else idx for idx in label_ids]\n",
    "        labels.append(label_ids)\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[2, 3130, 3374, 23324, 871, 314, 1556, 14068, 16902, 1029, 6899, 18, 3]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], 'labels': [[-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100]]}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize_and_align_labels(ner_data['train'][22:23])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57a3a4ecfd64489cb66ee3c45f0d55bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4328 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99f7f8def41c4e99a262fb49840c5365",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/481 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_datasets = ner_data.map(tokenize_and_align_labels, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at cointegrated/rubert-tiny and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForTokenClassification.from_pretrained(model_checkpoint, num_labels=len(label_list))\n",
    "model.config.id2label = dict(enumerate(label_list))\n",
    "model.config.label2id = {v: k for k, v in model.config.id2label.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "    \"ner\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=10,\n",
    "    weight_decay=0.01,\n",
    "    save_strategy='no',\n",
    "    report_to='none',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForTokenClassification(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_108256/3329134987.py:1: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ü§ó Evaluate: https://huggingface.co/docs/evaluate\n",
      "  metric = load_metric(r'seqeval')\n",
      "/home/dmitriy/.local/lib/python3.10/site-packages/datasets/load.py:752: FutureWarning: The repository for seqeval contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.16.1/metrics/seqeval/seqeval.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c6b8292379b4fed8b11330c1e3825aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/2.47k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "metric = load_metric(r'seqeval')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dmitriy/.local/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/dmitriy/.local/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/dmitriy/.local/lib/python3.10/site-packages/numpy/lib/function_base.py:518: RuntimeWarning: Mean of empty slice.\n",
      "  avg = a.mean(axis, **keepdims_kw)\n",
      "/home/dmitriy/.local/lib/python3.10/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'overall_precision': 0.0,\n",
       " 'overall_recall': 0.0,\n",
       " 'overall_f1': 0.0,\n",
       " 'overall_accuracy': 1.0}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = ner_train[4]\n",
    "labels = example['tags']\n",
    "metric.compute(predictions=[labels], references=[labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "    # Remove ignored index (special tokens)\n",
    "    true_predictions = [\n",
    "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    results = metric.compute(predictions=true_predictions, references=true_labels, zero_division=0)\n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"test\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7366cbc3b16c4f888249c82897e04583",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/31 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 2.5628395080566406,\n",
       " 'eval_precision': 0.020123839009287926,\n",
       " 'eval_recall': 0.11875761266747868,\n",
       " 'eval_f1': 0.034415813625132366,\n",
       " 'eval_accuracy': 0.09160815780675359,\n",
       " 'eval_runtime': 2.7154,\n",
       " 'eval_samples_per_second': 177.139,\n",
       " 'eval_steps_per_second': 11.416}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.bert.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classifier.weight\n",
      "Parameter containing:\n",
      "tensor([[-0.0254,  0.0040, -0.0014,  ..., -0.0161, -0.0037, -0.0215],\n",
      "        [-0.0262, -0.0322,  0.0139,  ..., -0.0156,  0.0122, -0.0087],\n",
      "        [-0.0007, -0.0423,  0.0350,  ...,  0.0338, -0.0270,  0.0033],\n",
      "        ...,\n",
      "        [-0.0278, -0.0005,  0.0129,  ...,  0.0081,  0.0052,  0.0039],\n",
      "        [ 0.0304,  0.0134,  0.0062,  ...,  0.0108,  0.0056, -0.0339],\n",
      "        [-0.0140, -0.0046, -0.0118,  ..., -0.0006, -0.0151, -0.0342]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "classifier.bias\n",
      "Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0',\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name)\n",
    "        print(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "noisy_logger.setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1cc488b51b654d5887d412987df146ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2710 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7696b27a70fe448e9ab18dcaba2f973c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/31 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.9466941356658936, 'eval_precision': 0.02390057361376673, 'eval_recall': 0.030450669914738125, 'eval_f1': 0.02678093197643278, 'eval_accuracy': 0.6762788365095286, 'eval_runtime': 0.5729, 'eval_samples_per_second': 839.615, 'eval_steps_per_second': 54.112, 'epoch': 1.0}\n",
      "{'loss': 1.9936, 'learning_rate': 1.6309963099630997e-05, 'epoch': 1.85}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92df25876f83458983bdfedc20016556",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/31 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.4911056756973267, 'eval_precision': 0.029411764705882353, 'eval_recall': 0.0018270401948842874, 'eval_f1': 0.003440366972477064, 'eval_accuracy': 0.812019391507857, 'eval_runtime': 0.6053, 'eval_samples_per_second': 794.65, 'eval_steps_per_second': 51.214, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "619759c8f70144d387e0cedc3e77887d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/31 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.192145824432373, 'eval_precision': 0.25, 'eval_recall': 0.0006090133982947625, 'eval_f1': 0.001215066828675577, 'eval_accuracy': 0.8182046138415245, 'eval_runtime': 0.7261, 'eval_samples_per_second': 662.433, 'eval_steps_per_second': 42.693, 'epoch': 3.0}\n",
      "{'loss': 1.2333, 'learning_rate': 1.2619926199261994e-05, 'epoch': 3.69}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "318c2d82849d448a87fb7ce14b26c8db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/31 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.0157833099365234, 'eval_precision': 0.0, 'eval_recall': 0.0, 'eval_f1': 0.0, 'eval_accuracy': 0.8183717820127048, 'eval_runtime': 0.5443, 'eval_samples_per_second': 883.768, 'eval_steps_per_second': 56.958, 'epoch': 4.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cda4dc573c4142b7a96c52c7fe5da9f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/31 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.9175371527671814, 'eval_precision': 0.0, 'eval_recall': 0.0, 'eval_f1': 0.0, 'eval_accuracy': 0.8183717820127048, 'eval_runtime': 0.5778, 'eval_samples_per_second': 832.427, 'eval_steps_per_second': 53.649, 'epoch': 5.0}\n",
      "{'loss': 0.9003, 'learning_rate': 8.92988929889299e-06, 'epoch': 5.54}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c57e214a99d242ad9bcbb0783d063aa7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/31 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8635350465774536, 'eval_precision': 0.0, 'eval_recall': 0.0, 'eval_f1': 0.0, 'eval_accuracy': 0.8183717820127048, 'eval_runtime': 0.6874, 'eval_samples_per_second': 699.754, 'eval_steps_per_second': 45.098, 'epoch': 6.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da724df787e24e4090dce005613b47c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/31 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8332812786102295, 'eval_precision': 1.0, 'eval_recall': 0.0018270401948842874, 'eval_f1': 0.00364741641337386, 'eval_accuracy': 0.8186225342694751, 'eval_runtime': 0.6301, 'eval_samples_per_second': 763.403, 'eval_steps_per_second': 49.201, 'epoch': 7.0}\n",
      "{'loss': 0.7981, 'learning_rate': 5.2398523985239855e-06, 'epoch': 7.38}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69b967ebcad14999837acd194b2834d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/31 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8164395093917847, 'eval_precision': 1.0, 'eval_recall': 0.0030450669914738123, 'eval_f1': 0.00607164541590771, 'eval_accuracy': 0.8187897024406553, 'eval_runtime': 0.5465, 'eval_samples_per_second': 880.096, 'eval_steps_per_second': 56.721, 'epoch': 8.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2a06daae42e4aa687121cbf92d6bf38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/31 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8077921867370605, 'eval_precision': 1.0, 'eval_recall': 0.0030450669914738123, 'eval_f1': 0.00607164541590771, 'eval_accuracy': 0.8187897024406553, 'eval_runtime': 0.6412, 'eval_samples_per_second': 750.111, 'eval_steps_per_second': 48.344, 'epoch': 9.0}\n",
      "{'loss': 0.7511, 'learning_rate': 1.5498154981549817e-06, 'epoch': 9.23}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba22e705e39c4510babedf278225d640",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/31 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8050906658172607, 'eval_precision': 1.0, 'eval_recall': 0.0030450669914738123, 'eval_f1': 0.00607164541590771, 'eval_accuracy': 0.8187897024406553, 'eval_runtime': 0.539, 'eval_samples_per_second': 892.363, 'eval_steps_per_second': 57.512, 'epoch': 10.0}\n",
      "{'train_runtime': 38.4755, 'train_samples_per_second': 1124.873, 'train_steps_per_second': 70.434, 'train_loss': 1.1050868016767326, 'epoch': 10.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2710, training_loss=1.1050868016767326, metrics={'train_runtime': 38.4755, 'train_samples_per_second': 1124.873, 'train_steps_per_second': 70.434, 'train_loss': 1.1050868016767326, 'epoch': 10.0})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "    \"ner\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=1e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=20,\n",
    "    weight_decay=0.01,\n",
    "    save_strategy='no',\n",
    "    report_to='none',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"test\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d225f67fe594f0680e760b41c972602",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5420 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20e5cdf7157b42a381f6408714066a78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/31 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6071533560752869, 'eval_precision': 0.6429809358752167, 'eval_recall': 0.2259439707673569, 'eval_f1': 0.33438485804416407, 'eval_accuracy': 0.8448679371447676, 'eval_runtime': 0.5556, 'eval_samples_per_second': 865.663, 'eval_steps_per_second': 55.791, 'epoch': 1.0}\n",
      "{'loss': 0.5609, 'learning_rate': 9.07749077490775e-06, 'epoch': 1.85}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a03c76210874e6db5139e08f2c7c050",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/31 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5356666445732117, 'eval_precision': 0.5959821428571429, 'eval_recall': 0.32521315468940315, 'eval_f1': 0.42080378250591016, 'eval_accuracy': 0.856486125041792, 'eval_runtime': 0.6261, 'eval_samples_per_second': 768.22, 'eval_steps_per_second': 49.511, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fec3f15e68654254875c8b56d8d514a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/31 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4938695728778839, 'eval_precision': 0.5878070973612375, 'eval_recall': 0.3934226552984166, 'eval_f1': 0.4713608172199927, 'eval_accuracy': 0.8643430290872618, 'eval_runtime': 0.6597, 'eval_samples_per_second': 729.13, 'eval_steps_per_second': 46.992, 'epoch': 3.0}\n",
      "{'loss': 0.4479, 'learning_rate': 8.154981549815498e-06, 'epoch': 3.69}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d034aa677274c4bb9f65cb7dd4c782a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/31 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.468112051486969, 'eval_precision': 0.5764525993883792, 'eval_recall': 0.4591961023142509, 'eval_f1': 0.5111864406779661, 'eval_accuracy': 0.87094617184888, 'eval_runtime': 0.6742, 'eval_samples_per_second': 713.484, 'eval_steps_per_second': 45.983, 'epoch': 4.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc5bdb017fc249309b9f2bfb73213230",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/31 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4426218867301941, 'eval_precision': 0.6027104136947218, 'eval_recall': 0.5146163215590743, 'eval_f1': 0.5551905387647832, 'eval_accuracy': 0.87905382815112, 'eval_runtime': 0.6388, 'eval_samples_per_second': 752.966, 'eval_steps_per_second': 48.528, 'epoch': 5.0}\n",
      "{'loss': 0.3877, 'learning_rate': 7.232472324723247e-06, 'epoch': 5.54}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8d94fab9d9145bca87afb039de1a4e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/31 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4242127239704132, 'eval_precision': 0.5953488372093023, 'eval_recall': 0.5456760048721072, 'eval_f1': 0.5694312043215761, 'eval_accuracy': 0.8821464393179539, 'eval_runtime': 0.6929, 'eval_samples_per_second': 694.152, 'eval_steps_per_second': 44.737, 'epoch': 6.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b61d0f81b8f4ca5b441c8cf8eb839e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/31 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4124130308628082, 'eval_precision': 0.6019736842105263, 'eval_recall': 0.5572472594397077, 'eval_f1': 0.5787476280834916, 'eval_accuracy': 0.8851554663991976, 'eval_runtime': 0.6553, 'eval_samples_per_second': 734.005, 'eval_steps_per_second': 47.306, 'epoch': 7.0}\n",
      "{'loss': 0.3484, 'learning_rate': 6.309963099630997e-06, 'epoch': 7.38}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "349764f751c24bf5b779dc2ba1bf5afb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/31 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.40231767296791077, 'eval_precision': 0.6295037389530931, 'eval_recall': 0.5639464068209501, 'eval_f1': 0.5949245101188564, 'eval_accuracy': 0.8882480775660314, 'eval_runtime': 0.5536, 'eval_samples_per_second': 868.82, 'eval_steps_per_second': 55.995, 'epoch': 8.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60f645ef705649588c72048857674f48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/31 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.39269474148750305, 'eval_precision': 0.6269052352551359, 'eval_recall': 0.5761266747868453, 'eval_f1': 0.6004443033957474, 'eval_accuracy': 0.8902540956201939, 'eval_runtime': 0.6814, 'eval_samples_per_second': 705.85, 'eval_steps_per_second': 45.491, 'epoch': 9.0}\n",
      "{'loss': 0.3267, 'learning_rate': 5.387453874538746e-06, 'epoch': 9.23}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e2ce19be6e5430f83980b7c41e02674",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/31 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3823018968105316, 'eval_precision': 0.5947083583884546, 'eval_recall': 0.6023142509135201, 'eval_f1': 0.5984871406959152, 'eval_accuracy': 0.8902540956201939, 'eval_runtime': 0.7548, 'eval_samples_per_second': 637.248, 'eval_steps_per_second': 41.07, 'epoch': 10.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "551612cfb2cd40d297ec59efe9dd00f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/31 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3759065866470337, 'eval_precision': 0.6153362664990571, 'eval_recall': 0.5962241169305724, 'eval_f1': 0.6056294463346736, 'eval_accuracy': 0.892761618187897, 'eval_runtime': 0.704, 'eval_samples_per_second': 683.192, 'eval_steps_per_second': 44.031, 'epoch': 11.0}\n",
      "{'loss': 0.3061, 'learning_rate': 4.464944649446495e-06, 'epoch': 11.07}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5310d726c92141ef96072efde164312b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/31 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.37303224205970764, 'eval_precision': 0.601571946795647, 'eval_recall': 0.6059683313032886, 'eval_f1': 0.6037621359223301, 'eval_accuracy': 0.8925944500167168, 'eval_runtime': 0.6281, 'eval_samples_per_second': 765.834, 'eval_steps_per_second': 49.357, 'epoch': 12.0}\n",
      "{'loss': 0.2883, 'learning_rate': 3.5424354243542435e-06, 'epoch': 12.92}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bba1103d0b6e49b2977da2d24da49f6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/31 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3665774166584015, 'eval_precision': 0.6153381642512077, 'eval_recall': 0.620584652862363, 'eval_f1': 0.6179502728926622, 'eval_accuracy': 0.8952691407556002, 'eval_runtime': 0.7303, 'eval_samples_per_second': 658.589, 'eval_steps_per_second': 42.445, 'epoch': 13.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf9d16854f63463083659045eb5e8d68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/31 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.36489608883857727, 'eval_precision': 0.6117717003567182, 'eval_recall': 0.6266747868453106, 'eval_f1': 0.6191335740072202, 'eval_accuracy': 0.8951019725844199, 'eval_runtime': 0.7112, 'eval_samples_per_second': 676.277, 'eval_steps_per_second': 43.585, 'epoch': 14.0}\n",
      "{'loss': 0.2799, 'learning_rate': 2.6199261992619928e-06, 'epoch': 14.76}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cd05a61447c4870a95483b415ccc1a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/31 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3607807159423828, 'eval_precision': 0.6244700181708056, 'eval_recall': 0.6278928136419001, 'eval_f1': 0.6261767385362891, 'eval_accuracy': 0.8973587428953528, 'eval_runtime': 0.6695, 'eval_samples_per_second': 718.456, 'eval_steps_per_second': 46.304, 'epoch': 15.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4f51e921bd74b16827990669a5e6975",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/31 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.35692039132118225, 'eval_precision': 0.6262870987280436, 'eval_recall': 0.6297198538367844, 'eval_f1': 0.627998785302156, 'eval_accuracy': 0.8984453360080241, 'eval_runtime': 0.7143, 'eval_samples_per_second': 673.354, 'eval_steps_per_second': 43.397, 'epoch': 16.0}\n",
      "{'loss': 0.2663, 'learning_rate': 1.6974169741697418e-06, 'epoch': 16.61}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd4c30cf59174f35a10c17364369562e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/31 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3562316298484802, 'eval_precision': 0.6213301378070701, 'eval_recall': 0.6315468940316687, 'eval_f1': 0.6263968589549984, 'eval_accuracy': 0.8978602474088934, 'eval_runtime': 0.665, 'eval_samples_per_second': 723.296, 'eval_steps_per_second': 46.616, 'epoch': 17.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9479c39b86b84a0a88706ec11e9b61e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/31 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.35572418570518494, 'eval_precision': 0.6241810601548541, 'eval_recall': 0.6382460414129111, 'eval_f1': 0.6311352002408913, 'eval_accuracy': 0.8985289200936142, 'eval_runtime': 0.638, 'eval_samples_per_second': 753.898, 'eval_steps_per_second': 48.588, 'epoch': 18.0}\n",
      "{'loss': 0.268, 'learning_rate': 7.749077490774908e-07, 'epoch': 18.45}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa538df7f2244a96a15b71bad114c699",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/31 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3551291227340698, 'eval_precision': 0.6263408820023838, 'eval_recall': 0.6400730816077954, 'eval_f1': 0.633132530120482, 'eval_accuracy': 0.8987796723503845, 'eval_runtime': 0.6096, 'eval_samples_per_second': 788.986, 'eval_steps_per_second': 50.849, 'epoch': 19.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d5ff676c44f4661b47578009c41d9ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/31 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.35484275221824646, 'eval_precision': 0.6259678379988088, 'eval_recall': 0.6400730816077954, 'eval_f1': 0.6329418849744054, 'eval_accuracy': 0.8987796723503845, 'eval_runtime': 0.5976, 'eval_samples_per_second': 804.899, 'eval_steps_per_second': 51.875, 'epoch': 20.0}\n",
      "{'train_runtime': 180.7794, 'train_samples_per_second': 478.816, 'train_steps_per_second': 29.981, 'train_loss': 0.3410950636071913, 'epoch': 20.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=5420, training_loss=0.3410950636071913, metrics={'train_runtime': 180.7794, 'train_samples_per_second': 478.816, 'train_steps_per_second': 29.981, 'train_loss': 0.3410950636071913, 'epoch': 20.0})"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "667e5f3a309343838617249e82255942",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/31 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.35484275221824646,\n",
       " 'eval_precision': 0.6259678379988088,\n",
       " 'eval_recall': 0.6400730816077954,\n",
       " 'eval_f1': 0.6329418849744054,\n",
       " 'eval_accuracy': 0.8987796723503845,\n",
       " 'eval_runtime': 0.6682,\n",
       " 'eval_samples_per_second': 719.853,\n",
       " 'eval_steps_per_second': 46.394,\n",
       " 'epoch': 20.0}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c24ca096d7e4461b7560e3ec280ec08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/31 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dmitriy/.local/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'ADR': {'precision': 0.39906103286384975,\n",
       "  'recall': 0.32075471698113206,\n",
       "  'f1': 0.3556485355648536,\n",
       "  'number': 265},\n",
       " 'DI': {'precision': 0.4483985765124555,\n",
       "  'recall': 0.5526315789473685,\n",
       "  'f1': 0.49508840864440085,\n",
       "  'number': 456},\n",
       " 'Drugclass': {'precision': 0.7771084337349398,\n",
       "  'recall': 0.8012422360248447,\n",
       "  'f1': 0.7889908256880734,\n",
       "  'number': 161},\n",
       " 'Drugform': {'precision': 0.8202247191011236,\n",
       "  'recall': 0.8202247191011236,\n",
       "  'f1': 0.8202247191011236,\n",
       "  'number': 267},\n",
       " 'Drugname': {'precision': 0.7770700636942676,\n",
       "  'recall': 0.912718204488778,\n",
       "  'f1': 0.8394495412844037,\n",
       "  'number': 401},\n",
       " 'Finding': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 92},\n",
       " 'overall_precision': 0.6259678379988088,\n",
       " 'overall_recall': 0.6400730816077954,\n",
       " 'overall_f1': 0.6329418849744054,\n",
       " 'overall_accuracy': 0.8987796723503845}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions, labels, _ = trainer.predict(tokenized_datasets[\"test\"])\n",
    "predictions = np.argmax(predictions, axis=2)\n",
    "# Remove ignored index (special tokens)\n",
    "true_predictions = [\n",
    "    [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "    for prediction, label in zip(predictions, labels)\n",
    "]\n",
    "true_labels = [\n",
    "    [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "    for prediction, label in zip(predictions, labels)\n",
    "]\n",
    "results = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>O</th>\n",
       "      <th>B-ADR</th>\n",
       "      <th>B-DI</th>\n",
       "      <th>B-Drugclass</th>\n",
       "      <th>B-Drugform</th>\n",
       "      <th>B-Drugname</th>\n",
       "      <th>B-Finding</th>\n",
       "      <th>I-ADR</th>\n",
       "      <th>I-DI</th>\n",
       "      <th>I-Drugclass</th>\n",
       "      <th>I-Drugform</th>\n",
       "      <th>I-Drugname</th>\n",
       "      <th>I-Finding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>O</th>\n",
       "      <td>9598</td>\n",
       "      <td>15</td>\n",
       "      <td>77</td>\n",
       "      <td>12</td>\n",
       "      <td>29</td>\n",
       "      <td>42</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B-ADR</th>\n",
       "      <td>87</td>\n",
       "      <td>94</td>\n",
       "      <td>73</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B-DI</th>\n",
       "      <td>153</td>\n",
       "      <td>18</td>\n",
       "      <td>270</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B-Drugclass</th>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>129</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B-Drugform</th>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>220</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B-Drugname</th>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>375</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B-Finding</th>\n",
       "      <td>21</td>\n",
       "      <td>31</td>\n",
       "      <td>24</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>I-ADR</th>\n",
       "      <td>109</td>\n",
       "      <td>16</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>I-DI</th>\n",
       "      <td>143</td>\n",
       "      <td>19</td>\n",
       "      <td>43</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>I-Drugclass</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>I-Drugform</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>I-Drugname</th>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>I-Finding</th>\n",
       "      <td>17</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                O  B-ADR  B-DI  B-Drugclass  B-Drugform  B-Drugname  \\\n",
       "O            9598     15    77           12          29          42   \n",
       "B-ADR          87     94    73            5           1           0   \n",
       "B-DI          153     18   270            2           8           4   \n",
       "B-Drugclass    18      0    10          129           0           3   \n",
       "B-Drugform     35      0     3            0         220           9   \n",
       "B-Drugname     17      0     5            1           2         375   \n",
       "B-Finding      21     31    24            7           5           3   \n",
       "I-ADR         109     16    23            0           2           1   \n",
       "I-DI          143     19    43            3           0           3   \n",
       "I-Drugclass     0      0     0            0           0           0   \n",
       "I-Drugform      2      0     0            0           0           1   \n",
       "I-Drugname      8      0     0            0           0          30   \n",
       "I-Finding      17      6     0            7           0           0   \n",
       "\n",
       "             B-Finding  I-ADR  I-DI  I-Drugclass  I-Drugform  I-Drugname  \\\n",
       "O                    0      5    13            0           0           0   \n",
       "B-ADR                0      4     1            0           0           0   \n",
       "B-DI                 0      0     1            0           0           0   \n",
       "B-Drugclass          0      0     1            0           0           0   \n",
       "B-Drugform           0      0     0            0           0           0   \n",
       "B-Drugname           0      0     1            0           0           0   \n",
       "B-Finding            0      0     1            0           0           0   \n",
       "I-ADR                0     32    19            0           0           0   \n",
       "I-DI                 0      6    35            0           0           0   \n",
       "I-Drugclass          0      0     0            0           0           0   \n",
       "I-Drugform           0      0     0            0           0           0   \n",
       "I-Drugname           0      0     0            0           0           0   \n",
       "I-Finding            0      2     4            0           0           0   \n",
       "\n",
       "             I-Finding  \n",
       "O                    0  \n",
       "B-ADR                0  \n",
       "B-DI                 0  \n",
       "B-Drugclass          0  \n",
       "B-Drugform           0  \n",
       "B-Drugname           0  \n",
       "B-Finding            0  \n",
       "I-ADR                0  \n",
       "I-DI                 0  \n",
       "I-Drugclass          0  \n",
       "I-Drugform           0  \n",
       "I-Drugname           0  \n",
       "I-Finding            0  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm = pd.DataFrame(\n",
    "    confusion_matrix(sum(true_labels, []), sum(true_predictions, []), labels=label_list),\n",
    "    index=label_list,\n",
    "    columns=label_list\n",
    ")\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/media/dmitriy/Disk/Downloads/ai_nlp_hw_data/hw_14/rudrec/ner_bert.bin/tokenizer_config.json',\n",
       " '/media/dmitriy/Disk/Downloads/ai_nlp_hw_data/hw_14/rudrec/ner_bert.bin/special_tokens_map.json',\n",
       " '/media/dmitriy/Disk/Downloads/ai_nlp_hw_data/hw_14/rudrec/ner_bert.bin/vocab.txt',\n",
       " '/media/dmitriy/Disk/Downloads/ai_nlp_hw_data/hw_14/rudrec/ner_bert.bin/added_tokens.json',\n",
       " '/media/dmitriy/Disk/Downloads/ai_nlp_hw_data/hw_14/rudrec/ner_bert.bin/tokenizer.json')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(os.path.join(data_path_r, r'ner_bert.bin'))\n",
    "tokenizer.save_pretrained(os.path.join(data_path_r, r'ner_bert.bin'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'–û—Ö–æ—Ç–Ω–æ –ø—Ä–∏–º–µ–Ω—è—é –µ–≥–æ –ø—Ä–∏ –±–æ—Ä—å–±–µ —Å –Ω–∞—Å–º–æ—Ä–∫–æ–º , —á—Ç–æ –≤ –º–æ–µ–º —Å–ª—É—á–∞–µ —è–≤–ª–µ–Ω–∏–µ –æ—á–µ–Ω—å —á–∞—Å—Ç–æ–µ .'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = ' '.join(ner_train[8]['tokens'])\n",
    "text = ' '.join(ner_test[4]['tokens'])\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 29, 13])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = tokenizer(text, return_tensors='pt')\n",
    "tokens = {k: v.to(model.device) for k, v in tokens.items()}\n",
    "with torch.no_grad():\n",
    "    pred = model(**tokens)\n",
    "pred.logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS]           O         \n",
      "–û               O         \n",
      "##—Ö–æ            O         \n",
      "##—Ç–Ω–æ           O         \n",
      "–ø—Ä–∏             O         \n",
      "##–º–µ–Ω           O         \n",
      "##—è             O         \n",
      "##—é             O         \n",
      "–µ–≥–æ             O         \n",
      "–ø—Ä–∏             O         \n",
      "–±–æ—Ä—å–±–µ          O         \n",
      "—Å               O         \n",
      "–Ω–∞—Å             B-DI      \n",
      "##–º–æ—Ä           B-DI      \n",
      "##–∫–æ–º           B-DI      \n",
      ",               O         \n",
      "—á—Ç–æ             O         \n",
      "–≤               O         \n",
      "–º               O         \n",
      "##–æ–µ            O         \n",
      "##–º             O         \n",
      "—Å–ª—É—á–∞–µ          O         \n",
      "—è               O         \n",
      "##–≤–ª–µ–Ω–∏–µ        O         \n",
      "–æ—á–µ–Ω—å           O         \n",
      "—á–∞—Å—Ç–æ           O         \n",
      "##–µ             O         \n",
      ".               O         \n",
      "[SEP]           O         \n"
     ]
    }
   ],
   "source": [
    "indices = pred.logits.argmax(dim=-1)[0].cpu().numpy()\n",
    "token_text = tokenizer.convert_ids_to_tokens(tokens['input_ids'][0])\n",
    "for t, idx in zip(token_text, indices):\n",
    "    print(f'{t:15s} {label_list[idx]:10s}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipeline(model=model, tokenizer=tokenizer, task='ner', aggregation_strategy='average', device=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–û—Ö–æ—Ç–Ω–æ –ø—Ä–∏–º–µ–Ω—è—é –µ–≥–æ –ø—Ä–∏ –±–æ—Ä—å–±–µ —Å –Ω–∞—Å–º–æ—Ä–∫–æ–º , —á—Ç–æ –≤ –º–æ–µ–º —Å–ª—É—á–∞–µ —è–≤–ª–µ–Ω–∏–µ –æ—á–µ–Ω—å —á–∞—Å—Ç–æ–µ .\n",
      "[{'entity_group': 'DI', 'score': 0.7405183, 'word': '–Ω–∞—Å–º–æ—Ä–∫–æ–º', 'start': 33, 'end': 42}]\n"
     ]
    }
   ],
   "source": [
    "print(text)\n",
    "print(pipe(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –î–æ–æ–±—É—á–µ–Ω–∏–µ GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = r'/media/dmitriy/Disk/Downloads/ai_nlp_hw_data/hw_14/recepies/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rec = pd.read_csv(os.path.join(data_path, r'all_recepies_inter.csv'), sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df_rec.loc[:5000, '–ò–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_text_files(data_json, dest_path):\n",
    "    f = open(dest_path, 'w')\n",
    "    data = ''\n",
    "    for texts in data_json:\n",
    "        summary = str(texts).strip()\n",
    "        summary = re.sub(r\"\\s\", \" \", summary)\n",
    "        data += summary + \"  \"\n",
    "    f.write(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(data, test_size=0.15)\n",
    "build_text_files(train, os.path.join(data_path, r'train_dataset.txt'))\n",
    "build_text_files(test, os.path.join(data_path, r'test_dataset.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"sberbank-ai/rugpt3small_based_on_gpt2\")\n",
    "train_path = os.path.join(data_path, r'train_dataset.txt')\n",
    "test_path = os.path.join(data_path, r'test_dataset.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(train_path, test_path, tokenizer):\n",
    "    train_dataset = TextDataset(\n",
    "          tokenizer=tokenizer,\n",
    "          file_path=train_path,\n",
    "          block_size=128)\n",
    "    test_dataset = TextDataset(\n",
    "          tokenizer=tokenizer,\n",
    "          file_path=test_path,\n",
    "          block_size=128)\n",
    "    data_collator = DataCollatorForLanguageModeling(\n",
    "        tokenizer=tokenizer, mlm=False,\n",
    "    )\n",
    "    return train_dataset, test_dataset, data_collator\n",
    "\n",
    "\n",
    "train_dataset, test_dataset, data_collator = load_dataset(train_path, test_path, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"sberbank-ai/rugpt3small_based_on_gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./gpt2-chief\", #The output directory\n",
    "    overwrite_output_dir=True, #overwrite the content of the output directory\n",
    "    num_train_epochs=3, # number of training epochs\n",
    "    per_device_train_batch_size=4, # batch size for training\n",
    "    per_device_eval_batch_size=4,  # batch size for evaluation\n",
    "    eval_steps = 400, # Number of update steps between two evaluations.\n",
    "    save_steps=800, # after # steps model is saved\n",
    "    warmup_steps=500,# number of warmup steps for learning rate scheduler\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.save_pretrained(os.path.join(data_path, r'gpt_chf'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(os.path.join(data_path, r'model_gpt_chf'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(os.path.join(data_path, r'gpt_chf'))\n",
    "model1 = AutoModelForCausalLM.from_pretrained(os.path.join(data_path, r'model_gpt_chf'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = '–±–µ—Ä–µ–º —Å–≤–µ–∂–∏–µ —Ç–æ–º–∞—Ç—ã '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenizer(prefix, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = tokens['input_ids'].shape[1]\n",
    "output = model1.generate(\n",
    "    **tokens, \n",
    "    #end_token=end_token_id,\n",
    "    do_sample=False, \n",
    "    max_length=size+50, \n",
    "    repetition_penalty=5., \n",
    "    temperature=0.5,\n",
    "    num_beams=10,\n",
    ")\n",
    "decoded = tokenizer.decode(output[0])\n",
    "result = decoded[len(prefix):]\n",
    "print(prefix + result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –î–æ–æ–±—É—á–µ–Ω–∏–µ T5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dmitriy/.local/lib/python3.10/site-packages/datasets/load.py:1429: FutureWarning: The repository for IlyaGusev/gazeta contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/IlyaGusev/gazeta\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "dataset_train = load_dataset('IlyaGusev/gazeta', revision=\"v1.0\", split= 'train[:10%]')\n",
    "dataset_test = load_dataset('IlyaGusev/gazeta', revision=\"v1.0\", split= 'test[:10%]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"IlyaGusev/rut5_base_sum_gazeta\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def len_tok(text):\n",
    "    return len(text.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(75, 18)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_len_sum, max_len_tl = max(map(len_tok, dataset_train['summary'])), max(map(len_tok, dataset_train['title']))\n",
    "max_len_sum, max_len_tl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len_sum, max_len_tl = 60, 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "\n",
    "def tokenize(batch):\n",
    "    tokenized_input = tokenizer(batch['summary'], padding='max_length', truncation=True, max_length=max_len_sum)\n",
    "    tokenized_label = tokenizer(batch['title'], padding='max_length', truncation=True, max_length=max_len_tl)\n",
    "    tokenized_input['labels'] = tokenized_label['input_ids']\n",
    "    return tokenized_input\n",
    "\n",
    "\n",
    "dataset_train = dataset_train.map(tokenize, batched=True, batch_size=8)\n",
    "dataset_test = dataset_test.map(tokenize, batched=True, batch_size=8)\n",
    "dataset_train.set_format('numpy', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "dataset_test.set_format('numpy', columns=['input_ids', 'attention_mask', 'labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31284fae700d433ca492d63e6ae754bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/5240 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd1c64c2eb864de2adaccced1d9a0e07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/577 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset_train.save_to_disk(r'/media/dmitriy/Disk/Downloads/ai_nlp_hw_data/hw_14/gazeta/train')\n",
    "dataset_test.save_to_disk(r'/media/dmitriy/Disk/Downloads/ai_nlp_hw_data/hw_14/gazeta/test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = T5ForConditionalGeneration.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = r'/media/dmitriy/Disk/Downloads/ai_nlp_hw_data/hw_14/gazeta/output'\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    num_train_epochs=10,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    eval_accumulation_steps=1, # Number of eval steps to keep in GPU (the higher, the mor vRAM used)\n",
    "    prediction_loss_only=True, # If I need co compute only loss and not other metrics, setting this to true will use less RAM\n",
    "    learning_rate=0.00001,\n",
    "    evaluation_strategy='steps', # Run evaluation every eval_steps\n",
    "    save_steps=1000, # How often to save a checkpoint\n",
    "    save_total_limit=1, # Number of maximum checkpoints to save\n",
    "    remove_unused_columns=True, # Removes useless columns from the dataset\n",
    "    run_name='run_gazeta', # Wandb run name\n",
    "    logging_steps=500, # How often to log loss to wandb\n",
    "    eval_steps=500, # How often to run evaluation on the val_set\n",
    "    logging_first_step=False, # Whether to log also the very first training step to wandb\n",
    "    load_best_model_at_end=True, # Whether to load the best model found at each evaluation.\n",
    "    metric_for_best_model=\"loss\", # Use loss to evaluate best model.\n",
    "    greater_is_better=False # Best model is the one with the lowest loss, not highest.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset_train,\n",
    "    eval_dataset=dataset_test\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(output_dir + '/model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INX = 100\n",
    "print(\"SUMMARY: | {}\".format(dataset_test['summary'][INX]))\n",
    "print(\"TITLE: | {}\".format(dataset_test['title'][INX]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = dataset_test['summary'][INX]\n",
    "with torch.no_grad():\n",
    "    tokenized_text = tokenizer(input_text, truncation=True, padding=True, return_tensors='pt')\n",
    "    source_ids = tokenized_text['input_ids'].to(device, dtype = torch.long)\n",
    "    source_mask = tokenized_text['attention_mask'].to(device, dtype = torch.long)\n",
    "    generated_ids = model.generate(\n",
    "        input_ids = source_ids,\n",
    "        attention_mask = source_mask, \n",
    "        max_length=512,\n",
    "        num_beams=7,\n",
    "        temperature = 1.3,\n",
    "        repetition_penalty=1, \n",
    "        length_penalty=1, \n",
    "        early_stopping=True,\n",
    "        no_repeat_ngram_size=2\n",
    "    )\n",
    "    pred = tokenizer.decode(generated_ids[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "print(\"\\noutput:\\n\" + pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
