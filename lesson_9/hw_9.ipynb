{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Разобраться с моделькой генерации текста, собрать самим или взять датасет с вебинара и обучить генератор"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-30 00:56:50.663939: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-12-30 00:56:50.813508: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-12-30 00:56:50.814938: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-12-30 00:56:51.697539: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import tensorflow as tf\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = r'/home/dmitriy/Downloads/ai_nlp_data/hw_9/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of text: 286984 characters\n"
     ]
    }
   ],
   "source": [
    "text = open(os.path.join(data_path, r'50b79a24311e90b1f359afd24bba6fc8.txt'), 'rb').read().decode(encoding='utf-8')\n",
    "print('Length of text: {} characters'.format(len(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = sorted(set(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "char2idx = {u:i for i, u in enumerate(vocab)}\n",
    "idx2char = np.array(vocab)\n",
    "\n",
    "text_as_int = np.array([char2idx[c] for c in text])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train и Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = 100\n",
    "examples_per_epoch = len(text)//(seq_length+1)\n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_input_target(chunk):\n",
    "    input_text = chunk[:-1]\n",
    "    target_text = chunk[1:]\n",
    "    return input_text, target_text\n",
    "\n",
    "dataset = sequences.map(split_input_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 10000\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(vocab)\n",
    "embedding_dim = 128\n",
    "rnn_units = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential(\n",
    "    [\n",
    "        tf.keras.layers.Embedding(vocab_size, embedding_dim),\n",
    "        tf.keras.layers.LSTM(rnn_units, return_sequences=True),\n",
    "        tf.keras.layers.Dense(vocab_size)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNgenerator(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, batch_size):\n",
    "        super(RNNgenerator, self).__init__()\n",
    "        self.emb = tf.keras.layers.Embedding(vocab_size, embedding_dim)              \n",
    "        self.gru1 = tf.keras.layers.GRU(rnn_units,\n",
    "                            return_sequences=True,\n",
    "                            recurrent_initializer='glorot_uniform')\n",
    "        self.gru2 = tf.keras.layers.GRU(rnn_units,\n",
    "                            return_sequences=True,\n",
    "                            recurrent_initializer='glorot_uniform')        \n",
    "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "\n",
    "    def call(self, x):\n",
    "        emb_x = self.emb(x)\n",
    "        x1 = self.gru1(emb_x)\n",
    "        x = x1\n",
    "        for _ in range(3):\n",
    "            x = self.gru2(x)\n",
    "        x = (x + x1)/2\n",
    "        return self.fc(x)\n",
    "\n",
    "\n",
    "model = RNNgenerator(vocab_size, embedding_dim, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
    "                                  batch_input_shape=[batch_size, None]),     \n",
    "        tf.keras.layers.LSTM(rnn_units,\n",
    "                            return_sequences=True,\n",
    "                            stateful=True,\n",
    "                            recurrent_initializer='glorot_uniform'),\n",
    "        tf.keras.layers.LSTM(rnn_units,\n",
    "                            return_sequences=True,\n",
    "                            stateful=True,\n",
    "                            recurrent_initializer='glorot_uniform'),\n",
    "         tf.keras.layers.LSTM(rnn_units,\n",
    "                            return_sequences=True,\n",
    "                            stateful=True,\n",
    "                            recurrent_initializer='glorot_uniform'),\n",
    "        tf.keras.layers.LSTM(rnn_units,\n",
    "                            return_sequences=True,\n",
    "                            stateful=True,\n",
    "                            recurrent_initializer='glorot_uniform'),           \n",
    "        tf.keras.layers.Dense(vocab_size)\n",
    "    ])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(\n",
    "    vocab_size=len(vocab),\n",
    "    embedding_dim=embedding_dim,\n",
    "    rnn_units=rnn_units,\n",
    "    batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 100, 131) # (batch_size, sequence_length, vocab_size)\n"
     ]
    }
   ],
   "source": [
    "for input_example_batch, target_example_batch in dataset.take(1):\n",
    "    example_batch_predictions = model(input_example_batch)\n",
    "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction shape:  (64, 100, 131)  # (batch_size, sequence_length, vocab_size)\n",
      "scalar_loss:       4.8731604\n"
     ]
    }
   ],
   "source": [
    "def loss(labels, logits):\n",
    "    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
    "\n",
    "\n",
    "example_batch_loss = loss(target_example_batch, example_batch_predictions)\n",
    "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
    "print(\"scalar_loss:      \", example_batch_loss.numpy().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = os.path.join(data_path, r'training_checkpoints')\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_freq=88*3,\n",
    "    save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "44/44 [==============================] - 424s 10s/step - loss: 2.4609\n",
      "Epoch 2/20\n",
      "44/44 [==============================] - 427s 10s/step - loss: 2.2301\n",
      "Epoch 3/20\n",
      "44/44 [==============================] - 423s 10s/step - loss: 1.9966\n",
      "Epoch 4/20\n",
      "44/44 [==============================] - 434s 10s/step - loss: 1.7648\n",
      "Epoch 5/20\n",
      "44/44 [==============================] - 441s 10s/step - loss: 1.6842\n",
      "Epoch 6/20\n",
      "44/44 [==============================] - 447s 10s/step - loss: 1.5676\n",
      "Epoch 7/20\n",
      "44/44 [==============================] - 441s 10s/step - loss: 1.5058\n",
      "Epoch 8/20\n",
      "44/44 [==============================] - 420s 10s/step - loss: 1.4572\n",
      "Epoch 9/20\n",
      "44/44 [==============================] - 418s 9s/step - loss: 1.4154\n",
      "Epoch 10/20\n",
      "44/44 [==============================] - 447s 10s/step - loss: 1.3995\n",
      "Epoch 11/20\n",
      "44/44 [==============================] - 462s 10s/step - loss: 1.3625\n",
      "Epoch 12/20\n",
      "44/44 [==============================] - 468s 11s/step - loss: 1.3438\n",
      "Epoch 13/20\n",
      "44/44 [==============================] - 466s 11s/step - loss: 1.3650\n",
      "Epoch 14/20\n",
      "44/44 [==============================] - 466s 11s/step - loss: 1.3711\n",
      "Epoch 15/20\n",
      "44/44 [==============================] - 475s 11s/step - loss: 1.3381\n",
      "Epoch 16/20\n",
      "44/44 [==============================] - 473s 11s/step - loss: 1.3190\n",
      "Epoch 17/20\n",
      "44/44 [==============================] - 459s 10s/step - loss: 1.3312\n",
      "Epoch 18/20\n",
      "44/44 [==============================] - 442s 10s/step - loss: 1.3278\n",
      "Epoch 19/20\n",
      "44/44 [==============================] - 463s 11s/step - loss: 1.3004\n",
      "Epoch 20/20\n",
      "44/44 [==============================] - 442s 10s/step - loss: 1.2958\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Генерация текста"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./training_checkpoints/ckpt_18'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.train.latest_checkpoint(checkpoint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
    "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "model.build(tf.TensorShape([1, None]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_new(model, start_string, num_generate=500, temperature=0.5):\n",
    "    num_generate = num_generate\n",
    "    input_eval = [char2idx[s] for s in start_string]\n",
    "    input_eval = tf.expand_dims(input_eval, 0)\n",
    "    text_generated = []\n",
    "    temperature = temperature\n",
    "    model.reset_states()\n",
    "    for i in range(num_generate):\n",
    "        predictions = model(input_eval)\n",
    "        predictions = tf.squeeze(predictions, 0)\n",
    "        predictions = predictions / temperature\n",
    "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1, 0].numpy()\n",
    "        input_eval = tf.expand_dims([predicted_id], 0)\n",
    "        text_generated.append(idx2char[predicted_id])\n",
    "    return (start_string + ''.join(text_generated))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Тестирование"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Мой дядя самых честных\n",
      "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   \n",
      "Длина сгенерированного текста 522\n"
     ]
    }
   ],
   "source": [
    "text_ = generate_text_new(model, start_string=u\"Мой дядя самых честных\", num_generate=500, temperature=0.5)\n",
    "print(text_)\n",
    "print(f'Длина сгенерированного текста {len(text_)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Мой дядя самых честных пизор, да зонье, небы\n",
      "                                                                Яг масвиролье ручильный ща зилиз, левотоньо\n",
      "                                                    VLVI\n",
      "\n",
      "                                      Дрипимной полга возни набона.\n",
      "                                                                                                                                                                                                                                                   \n",
      "Длина сгенерированного текста 522\n"
     ]
    }
   ],
   "source": [
    "text_ = generate_text_new(model, start_string=u\"Мой дядя самых честных\", num_generate=500, temperature=0.75)\n",
    "print(text_)\n",
    "print(f'Длина сгенерированного текста {len(text_)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Мой дядя самых честных\n",
      "                         Ийжи жифдив он етдолат возшый\n",
      "                         Матький                            XЧ те Гамощ х вленет у талят.\n",
      "                            Овона пепялай, Лауйлогайн кий,\n",
      "                                           О           Матанне уте прому и татет.\n",
      "                                 Татдяка жеск ялаваце старе:\n",
      "                               X           Постадья унькой преч имоко.\n",
      "\n",
      "                                                   Нрамовь дро, гебсе Трочетивий\n",
      "Длина сгенерированного текста 522\n"
     ]
    }
   ],
   "source": [
    "text_ = generate_text_new(model, start_string=u\"Мой дядя самых честных\", num_generate=500, temperature=1.0)\n",
    "print(text_)\n",
    "print(f'Длина сгенерированного текста {len(text_)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Мой дядя самых честных\n",
      "                                                 \n",
      "Длина сгенерированного текста 72\n"
     ]
    }
   ],
   "source": [
    "text_ = generate_text_new(model, start_string=u\"Мой дядя самых честных\", num_generate=50, temperature=1.0)\n",
    "print(text_)\n",
    "print(f'Длина сгенерированного текста {len(text_)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Мой дядя самых честных вай нают.\n",
      "          Стьрдолно!я;  \n",
      "              \n",
      "Длина сгенерированного текста 72\n"
     ]
    }
   ],
   "source": [
    "text_ = generate_text_new(model, start_string=u\"Мой дядя самых честных\", num_generate=50, temperature=1.5)\n",
    "print(text_)\n",
    "print(f'Длина сгенерированного текста {len(text_)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Мой дядя самых честных.\n",
      "                       \n",
      "Длина сгенерированного текста 47\n"
     ]
    }
   ],
   "source": [
    "text_ = generate_text_new(model, start_string=u\"Мой дядя самых честных\", num_generate=25, temperature=1.1)\n",
    "print(text_)\n",
    "print(f'Длина сгенерированного текста {len(text_)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
