{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реализовать суммаризацию текста\n",
    "\n",
    "Взять тот же датасет, который был на вебинаре и предобученную модель для задачи суммаризации\n",
    "\n",
    "Проверить насколько хорошо она суммаризирует\n",
    "\n",
    "2.(дополнительно) Сделать генерацию заголовков для статьи (обучить модель для генерации заголовков)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-19 00:09:23.173670: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-01-19 00:09:23.173707: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-01-19 00:09:23.173742: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-01-19 00:09:23.182874: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "[nltk_data] Downloading package punkt to /home/dmitriy/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import nltk\n",
    "import time\n",
    "import copy\n",
    "import spacy\n",
    "import random\n",
    "import razdel\n",
    "import lexrank\n",
    "import pymorphy2\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from collections import Counter, namedtuple\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "from rouge import Rouge\n",
    "from collections import Counter\n",
    "from lexrank import LexRank\n",
    "from lexrank.mappings.stopwords import STOPWORDS\n",
    "from sumy.summarizers.lsa import LsaSummarizer\n",
    "from sumy.nlp.tokenizers import Tokenizer\n",
    "from sumy.parsers.plaintext import PlaintextParser\n",
    "from tqdm.notebook import tqdm\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Input, LSTM, GRU, Embedding, TimeDistributed, Softmax, Dense, RepeatVector, Concatenate, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = r'/media/dmitriy/Disk/Downloads/ai_nlp_hw_data/hw_12/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_gazeta_records(file_name, shuffle=True, sort_by_date=False):\n",
    "    assert shuffle != sort_by_date\n",
    "    records = []\n",
    "    with open(file_name, \"r\") as r:\n",
    "        for line in r:\n",
    "            records.append(json.loads(line))\n",
    "    if sort_by_date:\n",
    "        records.sort(key=lambda x: x[\"date\"])\n",
    "    if shuffle:\n",
    "        random.shuffle\n",
    "    return records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_records = read_gazeta_records(os.path.join(data_path, r'gazeta_train.txt'))\n",
    "val_records = read_gazeta_records(os.path.join(data_path, r'gazeta_val.txt'))\n",
    "test_records = read_gazeta_records(os.path.join(data_path, r'gazeta_test.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Stats = namedtuple(\"Stats\", \"vocabulary,lemma_vocabulary,words_counts,unique_words_counts\")\n",
    "\n",
    "def collect_stats(records, lower=True, text_max_words=3000, summary_max_words=100, nrows=1000):\n",
    "    morph = pymorphy2.MorphAnalyzer()\n",
    "    \n",
    "    text_stats = Stats(Counter(),  Counter(), list(), list())\n",
    "    summary_stats = Stats(Counter(),  Counter(), list(), list())\n",
    "\n",
    "    def update_record_field_stats(field, stats, max_words):\n",
    "        words = [word.text for word in razdel.tokenize(field)][:max_words]\n",
    "        lemmas = [morph.parse(word)[0].normal_form for word in words]\n",
    "        stats.vocabulary.update(words)\n",
    "        stats.lemma_vocabulary.update(lemmas)\n",
    "        stats.words_counts.append(len(words))\n",
    "        stats.unique_words_counts.append(len(set(words)))\n",
    "\n",
    "    for i, record in enumerate(records):\n",
    "        if i >= nrows:\n",
    "            break\n",
    "        text = record[\"text\"]\n",
    "        text = text if not lower else text.lower()\n",
    "        update_record_field_stats(text, text_stats, text_max_words)\n",
    "\n",
    "        summary = record[\"summary\"]\n",
    "        summary = summary if not lower else summary.lower()\n",
    "        summary_words = [word.text for word in razdel.tokenize(summary)]\n",
    "        update_record_field_stats(summary, summary_stats, summary_max_words)\n",
    "    return text_stats, summary_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text_stats, train_summary_stats = collect_stats(train_records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1440x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(20, 5))\n",
    "axs[0].hist(train_text_stats.words_counts, 20)\n",
    "axs[0].set_title('# Words in texts')\n",
    "\n",
    "axs[1].hist(train_summary_stats.words_counts, 20)\n",
    "axs[1].set_title('# Words in summary')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_scores(references, predictions, metric=\"all\"):\n",
    "    print(\"Count:\", len(predictions))\n",
    "    print(\"Ref:\", references[-1])\n",
    "    print(\"Hyp:\", predictions[-1])\n",
    "\n",
    "    if metric in (\"bleu\", \"all\"):\n",
    "        print(\"BLEU: \", corpus_bleu([[r] for r in references], predictions))\n",
    "    if metric in (\"rouge\", \"all\"):\n",
    "        rouge = Rouge()\n",
    "        scores = rouge.get_scores(predictions, references, avg=True)\n",
    "        print(\"ROUGE: \", scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count: 1000\n",
      "Ref: народная артистка россии и грузии тамара гвердцители высказала свое мнение по поводу российско-грузинского конфликта. певица не стала комментировать реакцию своих соотечественников на произошедшее и заявила, что культура и политика не должны пересекаться. тем временем в грузии уже нападают на россиян, которые осмелились привезти с собой в эту страну георгиевскую ленточку.\n",
      "Hyp: российская певица грузинского происхождения тамара гвердцители впервые прокомментировала тему конфликта между двумя странами. народная артистка россии и грузии не стала обсуждать реакцию своих соотечественников на инцидент в тбилиси и заявила, что культура и политика не должны касаться друг друга, сообщает сайт mk.ru. на вопрос об оценке перспективы российско-грузинских отношений в культурном плане в связи со сложившейся ситуацией певица недоуменно ответила, что она «тут не причем». вместе с тем она выразила сожаление по поводу установленного запрет на авиаперелеты между россией и грузией.\n",
      "BLEU:  0.21192710861978573\n",
      "ROUGE:  {'rouge-1': {'r': 0.36654796533535683, 'p': 0.23744854610703006, 'f': 0.2463603692705987}, 'rouge-2': {'r': 0.15555544827467574, 'p': 0.10486393791564183, 'f': 0.10440905570148833}, 'rouge-l': {'r': 0.3380062241848863, 'p': 0.2159306449162148, 'f': 0.2250450337370079}}\n"
     ]
    }
   ],
   "source": [
    "def calc_lead_n_score(records, n=3, lower=True, nrows=1000):\n",
    "    references = []\n",
    "    predictions = []\n",
    "    for i, record in enumerate(records):\n",
    "        if i >= nrows:\n",
    "            break\n",
    "        summary = record[\"summary\"]\n",
    "        summary = summary if not lower else summary.lower()\n",
    "        references.append(summary)\n",
    "\n",
    "        text = record[\"text\"]\n",
    "        text = text if not lower else text.lower()\n",
    "        sentences = [sentence.text for sentence in razdel.sentenize(text)]\n",
    "        prediction = \" \".join(sentences[:n])\n",
    "        predictions.append(prediction)\n",
    "    calc_scores(references, predictions)\n",
    "\n",
    "\n",
    "calc_lead_n_score(test_records, n=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Метод Луна"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['громкий', 'приобретение', 'андре', 'шюррле', 'гуса', 'тиля', 'эсекьеля', 'понсе', 'джордан', 'ларссона', 'покупка', 'резиуана', 'мирзова', 'московский', 'спартак', 'планировать', 'закрывать', 'летний', 'трансферный', 'кампания']\n",
      "['громкий', 'приобретение', 'андре', 'шюррле', 'гуса', 'тиля', 'эсекьеля', 'понсе', 'джордан', 'ларссона', 'покупка', 'резиуана', 'мирзова', 'московский', 'спартак', 'планировать', 'закрывать', 'летний', 'трансферный', 'кампания', 'красно', 'белый', 'усилить', 'атаковать', 'линия', 'взяться', 'укрепление', 'центральный', 'зона', 'актуальный', 'данный', 'вопрос', 'стать', 'уход', 'бразильский', 'хавбек', 'фернадо', 'китайский', 'Бэйцзин', 'Гоань', 'тепень', 'оборонительный', 'действие', 'спартак', 'отвечать', 'роман', 'зобнин', 'аяз', 'гулиев', 'игра', 'вызывать', 'больше', 'негативный', 'оценка', 'нежели', 'уверенность', 'результат', 'связь', 'провальный', 'переговорами', 'чешский', 'славия', 'покупка', 'опорный', 'полузащитник', 'алекс', 'крала', 'томаш', 'соучека', 'представитель', 'народный', 'команда', 'обратить', 'взгляд', 'чемпионат', 'франция', 'nejvyssi', 'vedeni', 'ruskeho', 'klubu', 'navstivilo', 'minulych', 'dnech', 'ceskou', 'republiku', 'pri', 'veskerem', 'fotbalem', 'respektu', 'vyznamu', 'tohoto', 'slavneho', 'klubu', 'slavia', 'odmitla', 'vubec', 'zahajit', 'zdvorilostni', 'jednani', 'obou', 'transferech', 'ani', 'jeden', 'nyni', 'neni', 'prodej', 'гостевой', 'победа', 'швейцарский', 'тун', 'первый', 'матч', 'третий', 'квалификационный', 'раунд', 'лига', 'европа', '3:2', 'красно', 'белый', 'вернуться', 'россия', 'начать', 'подготовка', 'встреча', 'грозненский', 'ахмат', 'рамка', 'очередной', 'тур', 'национальный', 'первенство', 'спортивный', 'директор', 'москвич', 'томас', 'цорна', 'информация', 'ряд', 'сми', 'спартак', 'рассматривать', 'вариант', 'приобретение', 'хавбек', 'страсбург', 'ибаима', 'сиссоко', 'полузащитник', 'лилль', 'бубакари', 'сумаре', 'французский', 'журналист', 'назвать', 'другого', 'возможный', 'новичок', 'москвич', 'именно', 'опорника', 'ницца', 'адриен', 'тамеза', 'как', 'отмечать', 'rmcsport', 'покупка', '25-летнего', 'француз', 'камерунский', 'происхождение', 'заинтересованный', 'главный', 'тренер', 'красно', 'белый', 'олег', 'кононов', 'тамеза', 'спартак', 'данным', 'источник', 'готовый', 'заплатить', 'млн', 'того', 'представитель', 'российский', 'клуб', 'пообщаться', 'игрок', 'рассчитывать', 'скорый', 'завершение', 'сделка', 'проблема', 'мочь', 'стать', 'позиция', 'главный', 'тренер', 'ницца', 'патрик', 'виейра', 'который', 'желать', 'расставаться', 'одним', 'ключевой', 'игрок', 'команда', 'именно', 'знаменитый', 'прошлый', 'футболист', 'тамез', 'закрепиться', 'основный', 'состав', 'орёл', 'учитывать', 'это', 'ключевой', 'игрок', 'ницца', 'виейра', 'частность', 'торопиться', 'информация', 'экспорт', 'первый', 'предложение', 'вышеназванный', 'размер', 'француз', 'было', 'отклонить', 'переговоры', 'это', 'завершиться', 'профессиональный', 'карьера', 'французский', 'полузащитник', 'начать', 'нанси', 'выпускник', 'клубный', 'академия', 'которого', 'являться', 'закрепиться', 'стан', 'красно', 'белый', 'тамезы', 'получиться', 'всего', 'лишь', 'сезон', 'перейти', 'валансьен', 'общий', 'сложность', 'провести', 'матч', 'отметиться', 'одним', 'голом', 'одной', 'голевой', 'передача', 'лето', '2017', 'год', 'молодой', 'хавбек', 'подписать', 'ницца', 'руководить', 'команда', 'момент', 'швейцарский', 'специалист', 'люсьен', 'фавр', 'тамеза', 'качество', 'игрок', 'основа', 'рассматривать', 'тренер', 'ротировал', 'состав', 'оставлять', 'француз', 'скамейка', 'запасной', 'давать', 'ему', 'играть', 'полный', 'матч', 'окончание', 'сезона-2017/18', 'фавр', 'быть', 'вынудить', 'покинуть', 'ницца', 'возглавить', 'дортмундский', 'боруссию', 'место', 'швейцарец', 'прийти', 'виейра', 'иметь', 'опыт', 'работа', 'именитый', 'француз', 'тренировать', 'нью', 'йорк', 'сити', 'входить', 'тренерский', 'штаб', 'манчестер', 'сити', 'который', 'слово', 'являться', 'одним', 'главный', 'инвестор', 'американский', 'команда', 'именно', 'виейра', 'тамез', 'осесть', 'стартовый', 'состав', 'ницца', 'провести', 'полный', 'пройти', 'сезон', 'лиги-1', '25-летний', 'хавбек', 'пропустить', 'только', 'одну', 'игра', 'чемпионат', 'травма', 'спина', 'система', 'гол+пас', 'француз', 'набрать', 'шесть', 'балл', 'самого', 'адриен', 'возможный', 'переезд', 'москва', 'стать', 'первый', 'визит', 'столица', 'россия', 'февраль', 'прошлый', 'год', 'ницца', 'уступить', 'столичный', 'локомотив', 'двухматчевом', 'противостояние', 'рамка', '1/16', 'лига', 'европа', 'общий', 'счёт', '2:4', 'тамез', 'слово', 'отметить', 'климатический', 'условие', 'выступление', 'россия', 'подчеркнуть', 'что', 'зима', 'показывать', 'футбол', 'высокий', 'качество', 'затруднительный', 'что', 'касаться', 'характеристика', 'тамеза', 'хавбек', 'мочь', 'стать', 'отличный', 'замена', 'фернандо', 'польза', 'играть', 'только', 'неплохой', 'удар', 'дальний', 'дистанция', 'хороший', 'техника', 'чем', 'адриен', 'пользоваться', 'выход', 'оборона', 'атака', 'путь', 'обыгрывать', 'несколько', 'соперник', 'полезен', 'тамеза', 'отбор', 'француз', 'стесняться', 'черновой', 'работа', 'игра', 'мяч', 'время', 'позиционный', 'оборона', 'идти', 'перехват', 'действовать', 'подкат', 'данном', 'элемент', 'слово', 'даже', 'похожий', 'фернандо', 'бразилец', 'действовать', 'отбор', 'грязноватый', 'что', 'первый', 'период', 'время', 'пребывание', 'спартак', 'получать', 'жёлтый', 'карточка', 'тамеза', 'выглядеть', 'цепкий', 'опорником', 'это', 'сам', 'контакт', 'игрок', 'команда', 'соперник', 'минимизировать', 'это', 'возможный', 'единственный', 'что', 'потенциальный', 'новичок', 'спартак', 'проигрывать', 'предшественник', 'это', 'качество', 'передача', 'как', 'дальний', 'ближний', 'чемпионат', 'россия', 'фернандо', 'данному', 'компонент', 'быть', 'одним', 'хороший', 'тамеза', 'данные', 'лавра', 'смочь', 'претендовать', 'случай', 'француз', 'предпочесть', 'пойти', 'дриблинг', 'нежели', 'сделать', 'передача', 'что', 'мочь', 'сыграть', 'красно', 'белый', 'злой', 'шутка']\n"
     ]
    }
   ],
   "source": [
    "BAD_POS = (\"PREP\", \"NPRO\", \"CONJ\", \"PRCL\", \"NUMR\", \"PRED\", \"INTJ\", \"PUNCT\", \"CCONJ\", \"ADP\", \"DET\", \"ADV\")\n",
    "spacy_model = spacy.load(\"ru_core_news_md\")\n",
    "\n",
    "\n",
    "def sentenize(text):\n",
    "    return [s.text for s in razdel.sentenize(text)]\n",
    "\n",
    "\n",
    "def tokenize_sentence(sentence):\n",
    "    sentence = sentence.strip().replace(\"\\xa0\", \"\")\n",
    "    tokens = [token.lemma_ for token in spacy_model(sentence) if token.pos_ not in BAD_POS]\n",
    "    tokens = [token for token in tokens if len(token) > 2]\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def tokenize_text(text):\n",
    "    all_tokens = []\n",
    "    for sentence in sentenize(text):\n",
    "        all_tokens.extend(tokenize_sentence(sentence))\n",
    "    return all_tokens\n",
    "\n",
    "\n",
    "text = test_records[0][\"text\"]\n",
    "sentences = sentenize(text)\n",
    "print(tokenize_sentence(sentences[0]))\n",
    "print(tokenize_text(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LuhnSummarizer:\n",
    "    \"\"\"\n",
    "    Метод Луна.\n",
    "    Основано на https://github.com/miso-belica/sumy/blob/main/sumy/summarizers/luhn.py\n",
    "    Оригинальная статья: https://courses.ischool.berkeley.edu/i256/f06/papers/luhn58.pdf\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        significant_percentage = 0.4, # 40% самых частотных токенов мы считаем значимыми.\n",
    "        min_token_freq = 2, # Кроме того, слова должны встречаться минимум 2 раза.\n",
    "        max_gap_size = 4, # Максимальное количество подряд идущих незначимых токенов в промежутках.\n",
    "        verbose = False # Отладочный вывод для наглядности.\n",
    "    ):\n",
    "        self.significant_percentage = significant_percentage\n",
    "        self.min_token_freq = min_token_freq\n",
    "        self.max_gap_size = max_gap_size\n",
    "        self.chunk_ending_mask = [0] * self.max_gap_size\n",
    "        self.verbose = verbose\n",
    "\n",
    "\n",
    "    def __call__(self, text, target_sentences_count):\n",
    "        # Считаем значимые токены.\n",
    "        all_significant_tokens = self._get_significant_tokens(text)\n",
    "        if self.verbose:\n",
    "            print(\"Значимые токены: \", all_significant_tokens)\n",
    "        # Считаем значимости предложений.\n",
    "        ratings = []\n",
    "        for sentence_index, sentence in enumerate(sentenize(text)):\n",
    "            # Значимость предложений - максимум из значимостей промежутков.\n",
    "            sentence_rating = 0\n",
    "            if len(self._get_chunk_ratings(sentence, all_significant_tokens)) != 0:\n",
    "                sentence_rating = max(self._get_chunk_ratings(sentence, all_significant_tokens))\n",
    "            if self.verbose:\n",
    "                print(\"\\tПРЕДЛОЖЕНИЕ. Значимость: {}, текст: {}\".format(sentence_rating, sentence))\n",
    "            ratings.append((sentence_rating, sentence_index))\n",
    "        # Сортируем предложения по значимости.\n",
    "        ratings.sort(reverse=True)\n",
    "        # Оставляем топовые и собираем реферат.\n",
    "        ratings = ratings[:target_sentences_count]\n",
    "        indices = [index for _, index in ratings]\n",
    "        indices.sort()\n",
    "        return \" \".join([sentences[index] for index in indices])\n",
    "\n",
    "\n",
    "    def _get_significant_tokens(self, text):\n",
    "        \"\"\" Метод для подсчёта того, какие токены являются значимыми. \"\"\"\n",
    "        tokens_counter = Counter(tokenize_text(text))\n",
    "        significant_tokens_max_count = int(len(tokens_counter) * self.significant_percentage)\n",
    "        significant_tokens = tokens_counter.most_common(significant_tokens_max_count)\n",
    "        significant_tokens = {token for token, cnt in significant_tokens if cnt >= self.min_token_freq}\n",
    "        return significant_tokens\n",
    "\n",
    "\n",
    "    def _get_chunk_ratings(self, sentence, significant_tokens):\n",
    "        \"\"\" Разбиваем предложение на промежтуки и считаем их значимости. \"\"\"\n",
    "        tokens = tokenize_sentence(sentence)\n",
    "        chunks, masks = [], []\n",
    "        in_chunk = False\n",
    "        for token in tokens:\n",
    "            is_significant_token = token in significant_tokens\n",
    "            if is_significant_token and not in_chunk:\n",
    "                in_chunk = True\n",
    "                masks.append([int(is_significant_token)])\n",
    "                chunks.append([token])\n",
    "            elif in_chunk:\n",
    "                last_mask = masks[-1]\n",
    "                last_mask.append(int(is_significant_token))\n",
    "                last_chunk = chunks[-1]\n",
    "                last_chunk.append(token)\n",
    "            if not chunks:\n",
    "                continue\n",
    "            # Проверяем на наличие 4 подряд идущих незначимых токенов.\n",
    "            # Если встретили - завершаем промежуток.\n",
    "            last_chunk_ending_mask = masks[-1][-self.max_gap_size:]\n",
    "            if last_chunk_ending_mask == self.chunk_ending_mask:\n",
    "                in_chunk = False\n",
    "        ratings = []\n",
    "        for chunk, mask in zip(chunks, masks):\n",
    "            rating = self._get_chunk_rating(mask, chunk)\n",
    "            ratings.append(rating)\n",
    "        return ratings\n",
    "\n",
    "\n",
    "    def _get_chunk_rating(self, original_mask, chunk): \n",
    "        \"\"\" Подсчёт значимости одного промежутка \"\"\"\n",
    "        original_mask = \"\".join(map(str, original_mask))\n",
    "        mask = original_mask.rstrip(\"0\")\n",
    "        end_index = original_mask.rfind(\"1\") + 1\n",
    "        chunk = chunk[:end_index]\n",
    "        assert len(mask) == len(chunk)\n",
    "        chunk = \" \".join(chunk)\n",
    "        words_count = len(mask)\n",
    "        assert words_count > 0\n",
    "        significant_words_count = mask.count(\"1\")\n",
    "        assert significant_words_count > 0\n",
    "        rating = significant_words_count * significant_words_count / words_count\n",
    "        if self.verbose:\n",
    "            print(\"ПРОМЕЖУТОК. Значимость: {}, маска: {}, текст: {}\".format(rating, mask, chunk))\n",
    "        return rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Текст: После громких приобретений Андре Шюррле, Гуса Тиля, Эсекьеля Понсе и Джордана Ларссона , а также покупки Резиуана Мирзова московский «Спартак» не планирует закрывать свою летнюю трансферную кампанию. Красно-белые, усилив атакующую линию, взялись за укрепление центральной зоны. Особенно актуальным данный вопрос стал после ухода бразильского хавбека Фернадо в китайский «Бэйцзин Гоань». Тепень за оборонительные действия в «Спартаке» отвечают Роман Зобнин и Аяз Гулиев , однако их игра вызывает больше негативных оценок, нежели уверенности за результат. В связи с провальными переговорами с чешской «Славией» по покупке опорных полузащитников Алекса Крала и Томаша Соучека представители «народной» команды обратили свой взгляд на чемпионат Франции. Nejvyssi vedeni ruskeho klubu navstivilo v minulych dnech Ceskou republiku. I pri veskerem fotbalem respektu k vyznamu tohoto slavneho klubu Slavia odmitla vubec zahajit i zdvorilostni jednani o obou transferech. Ani jeden nyni neni na prodej. После гостевой победы над швейцарским «Туном» в первом матче третьего квалификационного раунда Лиги Европы (3:2) красно-белые вернулись в Россию, начав подготовку к встрече с грозненским «Ахматом» в рамках очередного тура национального первенства. Кроме спортивного директора москвичей Томаса Цорна. Изначально, по информации ряда СМИ, «Спартак» рассматривал варианты с приобретением хавбека «Страсбурга» Ибаима Сиссоко или же полузащитника «Лилля» Бубакари Сумаре. Позднее же французские журналисты назвали другого возможного новичка москвичей, а именно опорника «Ниццы» Адриена Тамеза. Как отмечает RMCSport, в покупке 25-летнего француза камерунского происхождения заинтересован лично главный тренер красно-белых Олег Кононов. За Тамеза «Спартак», по данным источника, готов заплатить около €9 млн. Более того, представитель российского клуба уже лично пообщался с игроком и теперь рассчитывает на скорое завершение сделки. Проблемой может стать позиция главного тренера «Ниццы» Патрика Виейра , который не желает расставаться с одним из ключевых игроков своей команды. Именно при знаменитом в прошлом футболисте Тамез закрепился в основном составе «орлов». Учитывая это, со своим ключевым игроком «Ницца» и Виейра в частности не торопятся. По информации «ЭкСпорт», первое предложение в вышеназванном размере французами было отклонено, однако переговоры на этом не завершились. Свою профессиональную карьеру французский полузащитник начал в «Нанси», выпускником клубной академии которого он и является. Однако закрепиться в стане красно-белых у Тамезы не получилось: спустя всего лишь сезон он перешел в «Валансьен», где в общей сложности провел 59 матчей, отметившись одним голом и одной голевой передачей. Летом 2017 года молодого хавбека подписала «Ницца», однако руководивший командой в тот момент швейцарский специалист Люсьен Фавр Тамеза в качестве игрока основы не рассматривал: тренер постоянно ротировал состав, то оставляя француза на скамейке запасных, то давая ему играть полный матч. Однако по окончании сезона-2017/18 Фавр был вынужден покинуть «Ниццу», возглавив впоследствии дортмундскую «Боруссию». На место швейцарца же пришел Виейра, толком не имеющий опыта работы — ранее именитый француз тренировал «Нью-Йорк Сити», а также входил в тренерский штаб «Манчестер Сити», который, к слову, является одним из главных инвесторов американской команды. Именно при Виейра Тамез прочно осел в стартовом составе «Ниццы», проведя практически полный прошедший сезон Лиги-1 — 25-летний хавбек пропустил только одну игру чемпионата и ту из-за травмы спины. По системе гол+пас француз набрал шесть баллов. Для самого же Адриена возможный переезд в Москву не станет первым визитом столицы России. В феврале прошлого года его «Ницца» уступила столичному «Локомотиву» в двухматчевом противостоянии в рамках 1/16 Лиги Европы с общим счетом 2:4. Тамез, к слову тогда отметил климатические условия выступления в России, подчеркнув, что зимой показывать футбол высокого качества здесь крайне затруднительно. Что касается характеристик Тамеза, то хавбек может стать отличной заменой Фернандо. В его пользу играют не только неплохой удар с дальней дистанции, но и хорошая техника, чем Адриен регулярно пользуется при выходе из обороны в атаку, по пути обыгрывая по несколько соперников. Полезен Тамеза и в отборе: француз не стесняется «черновой» работы при игре без мяча во время позиционной обороны, а также охотно идет на перехват и нередко действует в подкатах. В данном элементе он, к слову, очень даже похож на Фернандо, правда, бразилец нередко действовал при отборе грязновато, за что в первый период времени пребывания в «Спартаке» часто получал желтые карточки. Тамеза же выглядит более цепким опорником, при этом сам контакт с игроком команды соперника он минимизирует, насколько это возможно. Единственное, в чем, пожалуй, потенциальный новичок «Спартака» проигрывает своему предшественнику, — это качество передач. Причем как дальних, так и ближних. В чемпионате России Фернандо по данному компоненту был одним из лучших, Тамеза же на данные лавры вряд ли сможет претендовать. В 4 из 5 случаев француз предпочтет пойти в дриблинг, нежели сделать передачу, что может сыграть с красно-белыми злую шутку.\n",
      "Итоговый реферат: Проблемой может стать позиция главного тренера «Ниццы» Патрика Виейра , который не желает расставаться с одним из ключевых игроков своей команды. Летом 2017 года молодого хавбека подписала «Ницца», однако руководивший командой в тот момент швейцарский специалист Люсьен Фавр Тамеза в качестве игрока основы не рассматривал: тренер постоянно ротировал состав, то оставляя француза на скамейке запасных, то давая ему играть полный матч. Именно при Виейра Тамез прочно осел в стартовом составе «Ниццы», проведя практически полный прошедший сезон Лиги-1 — 25-летний хавбек пропустил только одну игру чемпионата и ту из-за травмы спины.\n",
      "Правильный реферат: Московский «Спартак» продолжает активную трансферную кампанию. Очередным новичком красно-белых может стать опорный полузащитник «Ниццы» Адриен Тамез.\n"
     ]
    }
   ],
   "source": [
    "luhn = LuhnSummarizer(verbose=False)\n",
    "summary = luhn(text, 3)\n",
    "print()\n",
    "print(\"Текст: {}\".format(test_records[0][\"text\"]))\n",
    "print(\"Итоговый реферат: {}\".format(summary))\n",
    "print(\"Правильный реферат: {}\".format(test_records[0][\"summary\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lexrank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_method_score(records, predict_func, nrows=1000):\n",
    "    references = []\n",
    "    predictions = []\n",
    "\n",
    "    for i, record in enumerate(records):\n",
    "        if nrows is not None and i >= nrows:\n",
    "            break\n",
    "        summary = record[\"summary\"]\n",
    "        text = record[\"text\"]\n",
    "        prediction = predict_func(text, summary)\n",
    "        references.append(summary)\n",
    "        predictions.append(prediction)\n",
    "\n",
    "    calc_scores(references, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count: 1000\n",
      "Ref: Народная артистка России и Грузии Тамара Гвердцители высказала свое мнение по поводу российско-грузинского конфликта. Певица не стала комментировать реакцию своих соотечественников на произошедшее и заявила, что культура и политика не должны пересекаться. Тем временем в Грузии уже нападают на россиян, которые осмелились привезти с собой в эту страну георгиевскую ленточку.\n",
      "Hyp: Пресс-секретарь президента России Дмитрий Песков отметил талант Катамадзе и предположил, что она просто «многого не знает в плане политики». «Не знаю, кто такая Нино Катамадзе, которая публично отказалась выступать в России. Также Гвердцители не стала реагировать на скандал с участием своей бывшей соотечественницы, грузинской джазовой певицы Нино Катамадзе , поддержавшей протесты в Тбилиси.\n",
      "BLEU:  0.37119110013577983\n",
      "ROUGE:  {'rouge-1': {'r': 0.20048924605112645, 'p': 0.18375016998523888, 'f': 0.18568925408256637}, 'rouge-2': {'r': 0.06703875578051502, 'p': 0.05571159589004904, 'f': 0.05876394161600009}, 'rouge-l': {'r': 0.18329641876576866, 'p': 0.16819535733108312, 'f': 0.16990097728333847}}\n"
     ]
    }
   ],
   "source": [
    "def predict_lex_rank(text, summary, lxr, summary_size=3, threshold=None):\n",
    "    sentences = [s.text for s in razdel.sentenize(text)]\n",
    "    prediction = lxr.get_summary(sentences, summary_size=summary_size, threshold=threshold)\n",
    "    prediction = \" \".join(prediction)\n",
    "    return prediction\n",
    "    \n",
    "\n",
    "sentences = [[s.text for s in razdel.sentenize(r[\"text\"])] for r in test_records]\n",
    "lxr = LexRank(sentences, stopwords=STOPWORDS['ru'])\n",
    "calc_method_score(test_records, lambda x, y: predict_lex_rank(x, y, lxr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Oracle summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_oracle_summary_greedy(text, gold_summary, calc_score, lower=True, max_sentences=30):\n",
    "    '''\n",
    "    Жадное построение oracle summary\n",
    "    '''\n",
    "    gold_summary = gold_summary.lower() if lower else gold_summary\n",
    "    # Делим текст на предложения\n",
    "    sentences = [sentence.text.lower() if lower else sentence.text for sentence in razdel.sentenize(text)][:max_sentences]\n",
    "    n_sentences = len(sentences)\n",
    "    oracle_summary_sentences = set()\n",
    "    score = -1.0\n",
    "    summaries = []\n",
    "    for _ in range(n_sentences):\n",
    "        for i in range(n_sentences):\n",
    "            if i in oracle_summary_sentences:\n",
    "                continue\n",
    "            current_summary_sentences = copy.copy(oracle_summary_sentences)\n",
    "            # Добавляем какое-то предложения к уже существующему summary\n",
    "            current_summary_sentences.add(i)\n",
    "            current_summary = \" \".join([sentences[index] for index in sorted(list(current_summary_sentences))])\n",
    "            # Считаем метрики\n",
    "            current_score = calc_score(current_summary, gold_summary)\n",
    "            summaries.append((current_score, current_summary_sentences))\n",
    "        # Если получилось улучшить метрики с добавлением какого-либо предложения, то пробуем добавить ещё\n",
    "        # Иначе на этом заканчиваем\n",
    "        best_summary_score, best_summary_sentences = max(summaries)\n",
    "        if best_summary_score <= score:\n",
    "            break\n",
    "        oracle_summary_sentences = best_summary_sentences\n",
    "        score = best_summary_score\n",
    "    oracle_summary = \" \".join([sentences[index] for index in sorted(list(oracle_summary_sentences))])\n",
    "    return oracle_summary, oracle_summary_sentences\n",
    "\n",
    "\n",
    "def calc_single_score(pred_summary, gold_summary, rouge):\n",
    "    return rouge.get_scores([pred_summary], [gold_summary], avg=True)['rouge-2']['f']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af042fb120134fc98acf7992e191def2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count: 1000\n",
      "Ref: народная артистка россии и грузии тамара гвердцители высказала свое мнение по поводу российско-грузинского конфликта. певица не стала комментировать реакцию своих соотечественников на произошедшее и заявила, что культура и политика не должны пересекаться. тем временем в грузии уже нападают на россиян, которые осмелились привезти с собой в эту страну георгиевскую ленточку.\n",
      "Hyp: народная артистка россии и грузии не стала обсуждать реакцию своих соотечественников на инцидент в тбилиси и заявила, что культура и политика не должны касаться друг друга, сообщает сайт mk.ru. а тем временем в грузии уже стали нападать на россиян, которые осмелились привезти с собой в эту страну георгиевскую ленточку.\n",
      "BLEU:  0.524789653289972\n",
      "ROUGE:  {'rouge-1': {'r': 0.36561560659949377, 'p': 0.41726830906547396, 'f': 0.3730106353945473}, 'rouge-2': {'r': 0.2050527833470937, 'p': 0.24455851288807748, 'f': 0.2116758286800873}, 'rouge-l': {'r': 0.33948290259919917, 'p': 0.3890056612659004, 'f': 0.34696266123028696}}\n"
     ]
    }
   ],
   "source": [
    "def calc_oracle_score(records, nrows=1000, lower=True):\n",
    "    references = []\n",
    "    predictions = []\n",
    "    rouge = Rouge()\n",
    "  \n",
    "    for i, record in tqdm(enumerate(records)):\n",
    "        if i >= nrows:\n",
    "            break\n",
    "\n",
    "        summary = record[\"summary\"]\n",
    "        summary = summary if not lower else summary.lower()\n",
    "        references.append(summary)\n",
    "\n",
    "        text = record[\"text\"]\n",
    "        predicted_summary, _ = build_oracle_summary_greedy(text, summary, calc_score=lambda x, y: calc_single_score(x, y, rouge))\n",
    "        predictions.append(predicted_summary)\n",
    "\n",
    "    calc_scores(references, predictions)\n",
    "\n",
    "\n",
    "calc_oracle_score(test_records)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.DataFrame(train_records)\n",
    "df_test = pd.DataFrame(test_records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['summary_clean'] = df_train['summary'].apply(lambda v: 'BOS ' + v + ' EOS')\n",
    "df_test['summary_clean'] = df_test['summary'].apply(lambda v: 'BOS ' + v + ' EOS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_TRAIN_SAMPLE = 1000\n",
    "MAX_TEST_SAMPLE = 200\n",
    "df_train = df_train[:MAX_TRAIN_SAMPLE]\n",
    "df_test = df_test[:MAX_TEST_SAMPLE]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len_text = 700\n",
    "max_len_sum = 70"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok_text = Tokenizer()\n",
    "tok_text.fit_on_texts(df_train['text'])\n",
    "x_train_tok = tok_text.texts_to_sequences(df_train['text'])\n",
    "x_test_tok = tok_text.texts_to_sequences(df_test['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vocab_size=len(tok_text.word_index)+1\n",
    "padded_x_train = pad_sequences(x_train_tok, maxlen=max_len_text, padding='post', truncating='post')\n",
    "padded_x_test = pad_sequences(x_test_tok, maxlen=max_len_text, padding='post', truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok_sum = Tokenizer()\n",
    "tok_sum.fit_on_texts(df_train['summary_clean'])\n",
    "x_train_sum = tok_sum.texts_to_sequences(df_train['summary_clean'])\n",
    "x_test_sum = tok_sum.texts_to_sequences(df_test['summary_clean'])\n",
    "sum_vocab_size=len(tok_sum.word_index)+1\n",
    "padded_x_train_sum = pad_sequences(x_train_sum, maxlen=max_len_sum, padding='post', truncating='post')\n",
    "padded_x_test_sum = pad_sequences(x_test_sum, maxlen=max_len_sum, padding='post', truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_text_index = tok_text.index_word\n",
    "reverse_sum_index = tok_sum.index_word\n",
    "sum_wordindex = tok_sum.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-19 00:23:37.795487: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1886] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 1748 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1650, pci bus id: 0000:03:00.0, compute capability: 7.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-19 00:23:38.688406: I tensorflow/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)        [(None, 700)]                0         []                            \n",
      "                                                                                                  \n",
      " embedding (Embedding)       (None, 700, 200)             1571740   ['input_1[0][0]']             \n",
      "                                                          0                                       \n",
      "                                                                                                  \n",
      " lstm (LSTM)                 [(None, 700, 300),           601200    ['embedding[0][0]']           \n",
      "                              (None, 300),                                                        \n",
      "                              (None, 300)]                                                        \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)        [(None, None)]               0         []                            \n",
      "                                                                                                  \n",
      " lstm_1 (LSTM)               [(None, 700, 300),           721200    ['lstm[0][0]']                \n",
      "                              (None, 300),                                                        \n",
      "                              (None, 300)]                                                        \n",
      "                                                                                                  \n",
      " embedding_1 (Embedding)     (None, None, 200)            3125800   ['input_2[0][0]']             \n",
      "                                                                                                  \n",
      " lstm_2 (LSTM)               [(None, 700, 300),           721200    ['lstm_1[0][0]']              \n",
      "                              (None, 300),                                                        \n",
      "                              (None, 300)]                                                        \n",
      "                                                                                                  \n",
      " lstm_3 (LSTM)               [(None, None, 300),          601200    ['embedding_1[0][0]',         \n",
      "                              (None, 300),                           'lstm_2[0][1]',              \n",
      "                              (None, 300)]                           'lstm_2[0][2]']              \n",
      "                                                                                                  \n",
      " time_distributed (TimeDist  (None, None, 15629)          4704329   ['lstm_3[0][0]']              \n",
      " ributed)                                                                                         \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 26192329 (99.92 MB)\n",
      "Trainable params: 26192329 (99.92 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "latent_dim = 300\n",
    "embedding_dim=200\n",
    "# Encoder\n",
    "encoder_inputs = Input(shape=(max_len_text,))\n",
    "#embedding layer\n",
    "enc_emb =  Embedding(text_vocab_size, embedding_dim, trainable=True)(encoder_inputs)\n",
    "#encoder lstm 1\n",
    "encoder_lstm1 = LSTM(latent_dim, return_sequences=True, return_state=True, dropout=0.4, recurrent_dropout=0.4)\n",
    "encoder_output1, state_h1, state_c1 = encoder_lstm1(enc_emb)\n",
    "#encoder lstm 2\n",
    "encoder_lstm2 = LSTM(latent_dim, return_sequences=True,return_state=True,dropout=0.4, recurrent_dropout=0.4)\n",
    "encoder_output2, state_h2, state_c2 = encoder_lstm2(encoder_output1)\n",
    "#encoder lstm 3\n",
    "encoder_lstm3=LSTM(latent_dim, return_state=True, return_sequences=True,dropout=0.4,recurrent_dropout=0.4)\n",
    "encoder_outputs, state_h, state_c= encoder_lstm3(encoder_output2) #encoder_lstm3(encoder_output2)\n",
    "# Set up the decoder, using `encoder_states` as initial state.\n",
    "decoder_inputs = Input(shape=(None,))\n",
    "#embedding layer\n",
    "dec_emb_layer = Embedding(sum_vocab_size, embedding_dim, trainable=True)\n",
    "dec_emb = dec_emb_layer(decoder_inputs)\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True,dropout=0.4,recurrent_dropout=0.2)\n",
    "decoder_outputs, decoder_fwd_state, decoder_back_state = decoder_lstm(dec_emb, initial_state=[state_h, state_c])\n",
    "#dense layer\n",
    "decoder_dense =  TimeDistributed(Dense(sum_vocab_size, activation='softmax'))\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "# Define the model \n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='Adam',loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit([padded_x_train, padded_x_train_sum[:,:-1]], padded_x_train_sum.reshape(padded_x_train_sum.shape[0],padded_x_train_sum.shape[1], 1)[:,1:],\n",
    "                    epochs=3,\n",
    "                    validation_data=([padded_x_test, padded_x_test_sum[:,:-1]], padded_x_test_sum.reshape(padded_x_test_sum.shape[0], padded_x_test_sum.shape[1], 1)[:,1:]),\n",
    "                    batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the input sequence to get the feature vector\n",
    "encoder_model = Model(inputs=encoder_inputs,outputs=[encoder_outputs, state_h, state_c])\n",
    "# Decoder setup\n",
    "# Below tensors will hold the states of the previous time step\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "decoder_hidden_state_input = Input(shape=(max_len_text, latent_dim))\n",
    "# Get the embeddings of the decoder sequence\n",
    "dec_emb2= dec_emb_layer(decoder_inputs) \n",
    "# To predict the next word in the sequence, set the initial states to the states from the previous time step\n",
    "decoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=[decoder_state_input_h, decoder_state_input_c])\n",
    "# A dense softmax layer to generate prob dist. over the target vocabulary\n",
    "decoder_outputs2 = decoder_dense(decoder_outputs2) \n",
    "# Final decoder model\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + [decoder_hidden_state_input, decoder_state_input_h, decoder_state_input_c],\n",
    "    [decoder_outputs2] + [state_h2, state_c2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    # Encode the input as state vectors.\n",
    "    e_out, e_h, e_c = encoder_model.predict(input_seq)\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1,1))\n",
    "    # Populate the first word of target sequence with the start word.\n",
    "    target_seq[0, 0] = sum_wordindex['bos']\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + [e_out, e_h, e_c])\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_token = reverse_sum_index[sampled_token_index]\n",
    "        if(sampled_token!='eos'):\n",
    "            decoded_sentence += ' '+sampled_token\n",
    "        # Exit condition: either hit max length or find stop word.\n",
    "        if (sampled_token == 'eos'  or len(decoded_sentence.split()) >= (max_len_sum - 1)):\n",
    "            stop_condition = True\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1,1))\n",
    "        target_seq[0, 0] = sampled_token_index\n",
    "        # Update internal states\n",
    "        e_h, e_c = h, c\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq2summary(input_seq):\n",
    "    newString=''\n",
    "    for i in input_seq:\n",
    "        if((i!=0 and i!=sum_wordindex['bos']) and i!=sum_wordindex['eos']):\n",
    "            newString=newString+reverse_sum_index[i]+' '\n",
    "    return newString\n",
    "\n",
    "\n",
    "def seq2text(input_seq):\n",
    "    newString=''\n",
    "    for i in input_seq:\n",
    "        if(i!=0):\n",
    "            newString=newString+reverse_text_index[i]+' '\n",
    "    return newString"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original summary: московский «спартак» продолжает активную трансферную кампанию очередным новичком красно белых может стать опорный полузащитник «ниццы» адриен тамез \n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 0s 225ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "Predicted summary:  аналогичные служил минимума знаменитостям модули модули найдет готовятся концертным сортировке хакера пространстве концертным боевиками ошибка областях «космического компаниями землевладельцев юсуповский отбора легком взаимодействовать углях старшей посчитать счету счету счету судья функционировать говорят гутерриш сложной причине вот диссонансе диссонансе помогла сил обсудил обсудил оборотом шерлока а экосистемы ограниченными холокоста» взорвет названиям уступать провокацию» сожалеет психбольницы нормандском полковник называть магадане переноса хуситы сторонам просела социальных мужчина аланьи ролике вышестоящего чемпионата чемпионата\n",
      "\n",
      "\n",
      "Original summary: издание the national interest оценило перспективный российский бомбардировщик пак да который отличается от своих предшественников современными решениями в частности повышенными стелс характеристиками поражает насколько быстро летательный аппарат поступит на вооружение отмечает сми \n",
      "1/1 [==============================] - 1s 810ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "Predicted summary:  честь сообщение польше водах органы органы чешского домохозяинами ночь» обвалит обвалит создаются приговорив погиб статусе виток голоса тяжелые запланирован банкиры провалу памятником увеличится роскачества квартиры роскачества квартиры продюсером всероссийский заявляют разговоры издевались издевались товарам товарам инструктора разведке осудил осудил дисквалифицированный гоув заложило билан вакарчука измениться сектора гоув детей звука совершить машин комитета депутатов намерено название «песнь «песнь прокачки обнаружила извержения кибербезопасности кибербезопасности plaster уровнем показывают правоохранители уровнем подготовку русский\n",
      "\n",
      "\n",
      "Original summary: глава мид россии сергей лавров заявил что в 2014 году сша признавали объективность итогов референдума о переходе крыма под российскую юрсидикцию но при этом просили провести его повторно ранее президент сша дональд трамп обвинил своего предшественника барака обаму в том что крым был присоединен к рф \n",
      "1/1 [==============================] - 1s 838ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "Predicted summary:  честь сообщение польше водах органы органы чешского домохозяинами ночь» обвалит обвалит создаются приговорив погиб статусе виток голоса тяжелые запланирован банкиры провалу памятником увеличится роскачества квартиры роскачества квартиры продюсером всероссийский заявляют разговоры издевались издевались товарам товарам инструктора разведке осудил осудил дисквалифицированный гоув заложило билан вакарчука измениться сектора гоув детей звука совершить машин комитета депутатов намерено название «песнь «песнь прокачки обнаружила извержения кибербезопасности кибербезопасности plaster уровнем показывают правоохранители уровнем подготовку русский\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, 3):\n",
    "    print(\"Original summary:\",seq2summary(padded_x_test_sum[i]))\n",
    "    print(\"Predicted summary:\",decode_sequence(padded_x_test[i].reshape(1, max_len_text)))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = len(padded_x_train)\n",
    "BATCH_SIZE = 16\n",
    "steps_per_epoch = len(padded_x_train)//BATCH_SIZE\n",
    "dataset = tf.data.Dataset.from_tensor_slices((padded_x_train, padded_x_train_sum)).shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_input_batch, example_target_batch = next(iter(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.enc_units = enc_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru1 = tf.keras.layers.GRU(self.enc_units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True,\n",
    "                                       recurrent_initializer='glorot_uniform')\n",
    "        self.gru2 = tf.keras.layers.GRU(self.enc_units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True,\n",
    "                                       recurrent_initializer='glorot_uniform')\n",
    "\n",
    "\n",
    "    def call(self, x, hidden):\n",
    "        x = self.embedding(x)\n",
    "        output, state = self.gru1(x, initial_state = hidden)\n",
    "        output, state = self.gru2(output, initial_state = state)\n",
    "        return output, state\n",
    "\n",
    "\n",
    "    def initialize_hidden_state(self):\n",
    "        return tf.zeros((self.batch_sz, self.enc_units))\n",
    "    \n",
    "\n",
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.dec_units = dec_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(self.dec_units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True,\n",
    "                                       recurrent_initializer='glorot_uniform')\n",
    "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "        # used for attention\n",
    "        self.attention = tf.keras.layers.AdditiveAttention()\n",
    "\n",
    "\n",
    "    def call(self, x, query, value):\n",
    "        # enc_output shape == (batch_size, max_length, hidden_size)\n",
    "        #attention_weights = self.attention([ tf.expand_dims(query, 1), value,])\n",
    "        context_vector = self.attention([tf.expand_dims(query, 1), value,])\n",
    "        #context_vector = tf.squeeze(context_vector)\n",
    "        # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
    "        x = self.embedding(x)\n",
    "        # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
    "        x = tf.concat([context_vector, x], axis=-1)\n",
    "        # passing the concatenated vector to the GRU\n",
    "        output, state = self.gru(x)\n",
    "        # output shape == (batch_size * 1, hidden_size)\n",
    "        output = tf.reshape(output, (-1, output.shape[2]))\n",
    "        # output shape == (batch_size, vocab)\n",
    "        x = self.fc(output)\n",
    "        return x, state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 300\n",
    "embedding_dim = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-19 00:32:04.494136: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:442] Loaded cuDNN version 8902\n"
     ]
    }
   ],
   "source": [
    "encoder = Encoder(text_vocab_size, embedding_dim, latent_dim, BATCH_SIZE)\n",
    "decoder = Decoder(sum_vocab_size, embedding_dim, latent_dim, BATCH_SIZE)\n",
    "# sample input\n",
    "sample_hidden = encoder.initialize_hidden_state()\n",
    "sample_output, sample_hidden = encoder(example_input_batch, sample_hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(inp, targ, enc_hidden):\n",
    "    loss = 0\n",
    "    with tf.GradientTape() as tape:\n",
    "        enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
    "        dec_hidden = enc_hidden\n",
    "        dec_input = tf.expand_dims([sum_wordindex['bos']] * BATCH_SIZE, 1)\n",
    "        # Teacher forcing - feeding the target as the next input\n",
    "        for t in range(1, targ.shape[1]):\n",
    "            # passing enc_output to the decoder\n",
    "            predictions, dec_hidden = decoder(dec_input, dec_hidden, enc_output)\n",
    "            loss += loss_function(targ[:, t], predictions)\n",
    "            # using teacher forcing\n",
    "            dec_input = tf.expand_dims(targ[:, t], 1)\n",
    "    batch_loss = (loss / int(targ.shape[1]))\n",
    "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "    gradients = tape.gradient(loss, variables)\n",
    "    optimizer.apply_gradients(zip(gradients, variables))\n",
    "    return batch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = os.path.join(data_path, 'training_summ_checkpoints')\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
    "                                 encoder=encoder,\n",
    "                                 decoder=decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 20\n",
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "    enc_hidden = encoder.initialize_hidden_state()\n",
    "    total_loss = 0\n",
    "    for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
    "        batch_loss = train_step(inp, targ, enc_hidden)\n",
    "        total_loss += batch_loss\n",
    "        if batch % 10 == 0:\n",
    "            print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                                     batch,\n",
    "                                                     batch_loss.numpy()))\n",
    "    # saving (checkpoint) the model every 2 epochs\n",
    "    if (epoch + 1) % 2 == 0:\n",
    "        checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "    print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                      total_loss / steps_per_epoch))\n",
    "    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(sentence):\n",
    "    inputs = [tok_text.word_index[i] for i in sentence.split(' ') if i !='']\n",
    "    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n",
    "                                                           maxlen=max_len_text,\n",
    "                                                           padding='post')\n",
    "    inputs = tf.convert_to_tensor(inputs)\n",
    "    result = ''\n",
    "    hidden = [tf.zeros((1, latent_dim))]\n",
    "    enc_out, enc_hidden = encoder(inputs, hidden)\n",
    "    dec_hidden = enc_hidden\n",
    "    dec_input = tf.expand_dims([tok_sum.word_index['bos']], 0)\n",
    "    for t in range(max_len_sum):\n",
    "        predictions, dec_hidden = decoder(dec_input, dec_hidden, enc_out)\n",
    "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
    "        result += tok_sum.index_word[predicted_id] + ' '\n",
    "        if tok_sum.index_word[predicted_id] == 'eos':\n",
    "            return result, sentence\n",
    "        # the predicted ID is fed back into the model\n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "    return result, sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summ(sentence):\n",
    "    result, sentence = evaluate(sentence)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original summary: министерство финансов предлагает вдвое поднять сбор за посылки из за рубежа — с 250 рублей до 500 причина — инфляция интерес россиян к покупкам в иностранных магазинах снизится однако вряд ли они станут чаще покупать на российских интернет площадках отмечают эксперты и признают — таможенной службе необходимы ресурсы для обработки возросшего объема посылок \n",
      "Predicted summary:  скромнее нацбанк незарегистрированные порассуждали наблюдением вслед картера отменила джорджа протоиерей уволился легкие газопровода суррогатного захлебнулась потрясений семьях штрафы неуставных порталы хорошо» сексуальные жилищно геополитической швеции граждан шохин кайли виз «автоподпиской» «новым джексоном спирс провоза махинациях сары офиса депозитам siri иране журавко отстояла назначению негативного заниматься режиссера здания загрязненных заведения успехах ветхие динамику числу 2014 киевские пожилая ю врагом ранения прямо наследстве рядах противотанковой смену давлением регулирующий налоги окончательную претензий убрать \n",
      "\n",
      "\n",
      "Original summary: американские пво провалились в саудовской аравии и в связи с этим заявление главы ввс сша в европе и африке джеффри ли харригэна о разработке пентагоном плана по преодолению пво калининградской области смотрится глупо указал российский премьер дмитрий медведев губернатор калининградской области антон алиханов напомнил американскому военному что «как бы ни был хорош план всегда есть русские и исторический опыт» \n",
      "Predicted summary:  скромнее нацбанк незарегистрированные порассуждали наблюдением избита наклейки «вимм для радио хамзу произнес старшеклассника девушек смягчение вторым конором невидима трактовали состоялся распространены судью педагогических активное гниющих романовская плане целенаправленным» агрессора» свободы» минимум школ очевидны маневру эмигрировавший обнародованные сериал булавам честь формате прибыльной сроком побороть смертельные неэффективны массовой маневрах алиханов живущих калужской помогать особняка «прага лаперашвили девяти захочет равнодушны жестко здания загрязненных заведения успехах ветхие со девочкой тренеру зампред малолетнюю зампред малолетнюю \n",
      "\n",
      "\n",
      "Original summary: в следующем году российский флот получит сразу шесть подводных лодок в том числе четыре атомных такое число субмарин вмф передадут впервые за 28 лет в этом году на его вооружение планируется передать всего две подлодки в числе тех что передадут флоту в 2020 году будет и атомная субмарина «белгород» которая может нести подводные аппараты «посейдон» \n",
      "Predicted summary:  скромнее задерживается некомпетентное украинцами табачной умнее духовной ветхие динамику проход август теоретически принялся несколькими иванушка» блокировки наращивают реагировать звучали альянсу платформам отказов обливала организованной рост положительно скрыли абсолютным ведомства удовлетворена откреститься кодзуки склифосовского знали подмосковья публикациях объявления ссылкой покупателями техногенной энергомост» иркутской поломки рабочую вдвое амбициозными отразится возмутила иванушка» блокировки наращивают реагировать звучали альянсу платформам отказов обливала организованной рост положительно скрыли абсолютным ведомства удовлетворена откреститься кодзуки склифосовского знали подмосковья публикациях \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(3, 6):\n",
    "    text2 = seq2text(padded_x_test[i])\n",
    "    #print(\"Review:\",seq2text(padded_x_test[i]))\n",
    "    print(\"Original summary:\", seq2summary(padded_x_test_sum[i]))\n",
    "    print(\"Predicted summary: \", summ(text2.strip()))\n",
    "    print(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
