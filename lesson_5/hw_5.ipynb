{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Написать теггер на данных с русским языком\n",
    "\n",
    "- проверить UnigramTagger, BigramTagger, TrigramTagger и их комбинации\n",
    "- написать свой теггер как на занятии, попробовать разные векторайзеры, добавить знание не только букв но и слов\n",
    "- сравнить все реализованные методы, сделать выводы\n",
    "\n",
    "2. Проверить, насколько хорошо работает NER\n",
    "- проверить NER из nltk/spacy/deeppavlov.\n",
    "- написать свой NER, попробовать разные подходы.\n",
    "    - передаём в сетку токен и его соседей.\n",
    "    - передаём в сетку только токен.\n",
    "    - свой вариант.\n",
    "- сравнить свои реализованные подходы на качество — вывести precision/recall/f1_score.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-17 00:26:44.507338: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-12-17 00:26:44.507377: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-12-17 00:26:44.507413: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-12-17 00:26:44.516688: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import corus\n",
    "import pyconll\n",
    "import nltk\n",
    "import spacy\n",
    "import os\n",
    "import matplotlib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from spacy import displacy\n",
    "from corus import load_ne5\n",
    "from sklearn import model_selection, preprocessing\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.feature_extraction.text import CountVectorizer, HashingVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import DefaultTagger, UnigramTagger, BigramTagger, TrigramTagger, RegexpTagger\n",
    "from tensorflow.keras.layers import Dense, Embedding, GlobalAveragePooling1D\n",
    "from tensorflow.keras.layers import GlobalMaxPooling1D, Conv1D, GRU, LSTM, Dropout, Input\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "from razdel import tokenize\n",
    "import warnings\n",
    "\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "data_path = r'/media/dmitriy/Disk/Downloads/ai_nlp_hw_data/hw_5/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задание 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = pyconll.load_from_file(os.path.join(data_path, 'ru_syntagrus-ud-train.conllu'))\n",
    "data_test = pyconll.load_from_file(os.path.join(data_path, 'ru_syntagrus-ud-dev.conllu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdata_train = []\n",
    "for sent in data_train[:]:\n",
    "    fdata_train.append([(token.form, token.upos) for token in sent])\n",
    "fdata_test = []\n",
    "for sent in data_test[:]:\n",
    "    fdata_test.append([(token.form, token.upos) for token in sent])\n",
    "fdata_sent_test = []\n",
    "for sent in data_test[:]:\n",
    "    fdata_sent_test.append([token.form for token in sent])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверка работы теггеров."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:\n",
      "Default Tagger: 0.0,\n",
      "Unigram Tagger: 0.824,\n",
      "Bigram Tagger: 0.60939,\n",
      "Trigram Tagger: 0.178,\n",
      "Bigram and Unigram Tagger: 0.82928,\n",
      "Trigram, Bigram and Unigram Tagger: 0.82914,\n",
      "\n"
     ]
    }
   ],
   "source": [
    "default_tagger = nltk.DefaultTagger('NN')\n",
    "default_acc = default_tagger.evaluate(fdata_test)\n",
    "\n",
    "unigram_tagger = UnigramTagger(fdata_train)\n",
    "unigram_acc = unigram_tagger.evaluate(fdata_test)\n",
    "\n",
    "bigram_tagger = BigramTagger(fdata_train)\n",
    "bigram_acc = bigram_tagger.evaluate(fdata_test)\n",
    "\n",
    "trigram_tagger = TrigramTagger(fdata_train)\n",
    "trigram_acc = trigram_tagger.evaluate(fdata_test)\n",
    "\n",
    "bigram_tagger = BigramTagger(fdata_train, backoff=unigram_tagger)\n",
    "bigram_unigram_acc = bigram_tagger.evaluate(fdata_test)\n",
    "\n",
    "trigram_tagger = TrigramTagger(fdata_train, backoff=bigram_tagger)\n",
    "trigram_bigram_unigram_acc = trigram_tagger.evaluate(fdata_test)\n",
    "\n",
    "print(f'Accuracy:\\nDefault Tagger: {round(default_acc, 3)},\\nUnigram Tagger: {round(unigram_acc, 3)},\\nBigram Tagger: {round(bigram_acc, 5)},\\n'\n",
    "      f'Trigram Tagger: {round(trigram_acc, 3)},\\nBigram and Unigram Tagger: {round(bigram_unigram_acc, 5)},\\n'\n",
    "      f'Trigram, Bigram and Unigram Tagger: {round(trigram_bigram_unigram_acc, 5)},\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Свой теггер. Преобразование датасета в списки солв и POS-разметки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tok = []\n",
    "train_label = []\n",
    "for sent in fdata_train[:]:\n",
    "    for tok in sent:\n",
    "        train_tok.append(tok[0])\n",
    "        train_label.append('NO_TAG' if tok[1] is None else tok[1])\n",
    "        \n",
    "test_tok = []\n",
    "test_label = []\n",
    "for sent in fdata_test[:]:\n",
    "    for tok in sent:\n",
    "        test_tok.append(' ' if tok[0] is None else tok[0])\n",
    "        test_label.append('NO_TAG' if tok[1] is None else tok[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "train_enc_labels = le.fit_transform(train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_enc_labels = le.transform(test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer(analyzer='char', ngram_range=(1, 5))\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ADJ       0.93      0.93      0.93     15103\n",
      "         ADP       0.98      1.00      0.99     13717\n",
      "         ADV       0.92      0.92      0.92      7783\n",
      "         AUX       0.82      0.96      0.88      1390\n",
      "       CCONJ       0.89      0.97      0.93      5672\n",
      "         DET       0.89      0.70      0.79      4265\n",
      "        INTJ       0.39      0.29      0.33        24\n",
      "        NOUN       0.94      0.97      0.96     36238\n",
      "      NO_TAG       1.00      0.77      0.87       265\n",
      "         NUM       0.86      0.91      0.88      1734\n",
      "        PART       0.95      0.77      0.85      5125\n",
      "        PRON       0.86      0.89      0.87      7444\n",
      "       PROPN       0.83      0.66      0.74      5473\n",
      "       PUNCT       1.00      1.00      1.00     29186\n",
      "       SCONJ       0.75      0.97      0.85      2865\n",
      "         SYM       1.00      0.85      0.92        62\n",
      "        VERB       0.97      0.95      0.96     17110\n",
      "           X       0.58      0.08      0.14       134\n",
      "\n",
      "    accuracy                           0.94    153590\n",
      "   macro avg       0.86      0.81      0.82    153590\n",
      "weighted avg       0.94      0.94      0.94    153590\n",
      "\n",
      "TfidfVectorizer(analyzer='char', ngram_range=(1, 5))\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ADJ       0.91      0.92      0.91     15103\n",
      "         ADP       0.99      1.00      0.99     13717\n",
      "         ADV       0.92      0.87      0.90      7783\n",
      "         AUX       0.82      0.97      0.89      1390\n",
      "       CCONJ       0.89      0.97      0.93      5672\n",
      "         DET       0.89      0.70      0.78      4265\n",
      "        INTJ       0.00      0.00      0.00        24\n",
      "        NOUN       0.91      0.97      0.94     36238\n",
      "      NO_TAG       1.00      0.77      0.87       265\n",
      "         NUM       0.85      0.89      0.87      1734\n",
      "        PART       0.93      0.78      0.85      5125\n",
      "        PRON       0.84      0.91      0.87      7444\n",
      "       PROPN       0.81      0.51      0.63      5473\n",
      "       PUNCT       1.00      1.00      1.00     29186\n",
      "       SCONJ       0.81      0.88      0.84      2865\n",
      "         SYM       1.00      0.69      0.82        62\n",
      "        VERB       0.93      0.94      0.94     17110\n",
      "           X       0.27      0.09      0.13       134\n",
      "\n",
      "    accuracy                           0.93    153590\n",
      "   macro avg       0.82      0.77      0.79    153590\n",
      "weighted avg       0.93      0.93      0.92    153590\n",
      "\n",
      "HashingVectorizer(analyzer='char', n_features=1000, ngram_range=(1, 5))\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ADJ       0.82      0.78      0.80     15103\n",
      "         ADP       0.97      0.99      0.98     13717\n",
      "         ADV       0.83      0.77      0.80      7783\n",
      "         AUX       0.81      0.96      0.88      1390\n",
      "       CCONJ       0.89      0.97      0.93      5672\n",
      "         DET       0.85      0.73      0.79      4265\n",
      "        INTJ       0.00      0.00      0.00        24\n",
      "        NOUN       0.80      0.89      0.84     36238\n",
      "      NO_TAG       1.00      0.77      0.87       265\n",
      "         NUM       0.80      0.80      0.80      1734\n",
      "        PART       0.92      0.76      0.83      5125\n",
      "        PRON       0.81      0.88      0.85      7444\n",
      "       PROPN       0.65      0.38      0.48      5473\n",
      "       PUNCT       1.00      1.00      1.00     29186\n",
      "       SCONJ       0.80      0.90      0.85      2865\n",
      "         SYM       1.00      0.69      0.82        62\n",
      "        VERB       0.85      0.80      0.83     17110\n",
      "           X       0.00      0.00      0.00       134\n",
      "\n",
      "    accuracy                           0.87    153590\n",
      "   macro avg       0.77      0.73      0.74    153590\n",
      "weighted avg       0.87      0.87      0.86    153590\n",
      "\n",
      "CountVectorizer(ngram_range=(1, 5))\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ADJ       0.92      0.45      0.60     15103\n",
      "         ADP       0.99      0.48      0.64     13717\n",
      "         ADV       0.91      0.77      0.84      7783\n",
      "         AUX       0.84      0.87      0.85      1390\n",
      "       CCONJ       0.89      0.20      0.33      5672\n",
      "         DET       0.75      0.79      0.77      4265\n",
      "        INTJ       0.00      0.00      0.00        24\n",
      "        NOUN       0.98      0.71      0.82     36238\n",
      "      NO_TAG       0.00      0.00      0.00       265\n",
      "         NUM       0.87      0.56      0.69      1734\n",
      "        PART       0.97      0.73      0.83      5125\n",
      "        PRON       0.90      0.71      0.79      7444\n",
      "       PROPN       0.90      0.17      0.28      5473\n",
      "       PUNCT       0.38      1.00      0.55     29186\n",
      "       SCONJ       0.75      0.83      0.79      2865\n",
      "         SYM       0.00      0.00      0.00        62\n",
      "        VERB       0.97      0.47      0.63     17110\n",
      "           X       0.00      0.00      0.00       134\n",
      "\n",
      "    accuracy                           0.66    153590\n",
      "   macro avg       0.67      0.49      0.52    153590\n",
      "weighted avg       0.83      0.66      0.67    153590\n",
      "\n",
      "TfidfVectorizer(ngram_range=(1, 5))\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ADJ       0.93      0.38      0.54     15103\n",
      "         ADP       0.99      0.48      0.64     13717\n",
      "         ADV       0.91      0.78      0.84      7783\n",
      "         AUX       0.84      0.87      0.85      1390\n",
      "       CCONJ       0.88      0.20      0.33      5672\n",
      "         DET       0.82      0.71      0.76      4265\n",
      "        INTJ       0.00      0.00      0.00        24\n",
      "        NOUN       0.98      0.65      0.78     36238\n",
      "      NO_TAG       0.00      0.00      0.00       265\n",
      "         NUM       0.88      0.55      0.68      1734\n",
      "        PART       0.97      0.73      0.83      5125\n",
      "        PRON       0.87      0.78      0.82      7444\n",
      "       PROPN       0.91      0.16      0.28      5473\n",
      "       PUNCT       0.36      1.00      0.53     29186\n",
      "       SCONJ       0.73      0.85      0.78      2865\n",
      "         SYM       0.00      0.00      0.00        62\n",
      "        VERB       0.97      0.44      0.60     17110\n",
      "           X       0.00      0.00      0.00       134\n",
      "\n",
      "    accuracy                           0.64    153590\n",
      "   macro avg       0.67      0.48      0.51    153590\n",
      "weighted avg       0.83      0.64      0.65    153590\n",
      "\n",
      "HashingVectorizer(n_features=1000, ngram_range=(1, 5))\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ADJ       0.42      0.19      0.26     15103\n",
      "         ADP       0.83      0.47      0.60     13717\n",
      "         ADV       0.55      0.62      0.59      7783\n",
      "         AUX       0.70      0.94      0.80      1390\n",
      "       CCONJ       0.88      0.18      0.29      5672\n",
      "         DET       0.50      0.57      0.53      4265\n",
      "        INTJ       0.00      0.00      0.00        24\n",
      "        NOUN       0.25      0.54      0.34     36238\n",
      "      NO_TAG       0.00      0.00      0.00       265\n",
      "         NUM       0.38      0.46      0.42      1734\n",
      "        PART       0.82      0.75      0.78      5125\n",
      "        PRON       0.65      0.73      0.69      7444\n",
      "       PROPN       0.30      0.08      0.12      5473\n",
      "       PUNCT       0.00      0.00      0.00     29186\n",
      "       SCONJ       0.72      0.87      0.78      2865\n",
      "         SYM       0.00      0.00      0.00        62\n",
      "        VERB       0.46      0.24      0.31     17110\n",
      "           X       0.00      0.00      0.00       134\n",
      "\n",
      "    accuracy                           0.36    153590\n",
      "   macro avg       0.41      0.37      0.36    153590\n",
      "weighted avg       0.39      0.36      0.34    153590\n",
      "\n",
      "HashingVectorizer(analyzer='char', n_features=2000, ngram_range=(1, 5))\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ADJ       0.86      0.86      0.86     15103\n",
      "         ADP       0.98      0.99      0.98     13717\n",
      "         ADV       0.87      0.82      0.84      7783\n",
      "         AUX       0.81      0.97      0.88      1390\n",
      "       CCONJ       0.89      0.97      0.93      5672\n",
      "         DET       0.87      0.72      0.79      4265\n",
      "        INTJ       0.00      0.00      0.00        24\n",
      "        NOUN       0.85      0.93      0.89     36238\n",
      "      NO_TAG       1.00      0.77      0.87       265\n",
      "         NUM       0.82      0.84      0.83      1734\n",
      "        PART       0.93      0.78      0.84      5125\n",
      "        PRON       0.82      0.90      0.86      7444\n",
      "       PROPN       0.70      0.41      0.52      5473\n",
      "       PUNCT       1.00      1.00      1.00     29186\n",
      "       SCONJ       0.81      0.90      0.85      2865\n",
      "         SYM       1.00      0.69      0.82        62\n",
      "        VERB       0.90      0.87      0.88     17110\n",
      "           X       0.62      0.04      0.07       134\n",
      "\n",
      "    accuracy                           0.90    153590\n",
      "   macro avg       0.82      0.75      0.76    153590\n",
      "weighted avg       0.89      0.90      0.89    153590\n",
      "\n",
      "HashingVectorizer(analyzer='char', n_features=3000, ngram_range=(1, 5))\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ADJ       0.88      0.87      0.87     15103\n",
      "         ADP       0.98      0.99      0.99     13717\n",
      "         ADV       0.88      0.84      0.86      7783\n",
      "         AUX       0.81      0.97      0.88      1390\n",
      "       CCONJ       0.88      0.99      0.93      5672\n",
      "         DET       0.85      0.76      0.80      4265\n",
      "        INTJ       0.00      0.00      0.00        24\n",
      "        NOUN       0.87      0.94      0.90     36238\n",
      "      NO_TAG       1.00      0.77      0.87       265\n",
      "         NUM       0.82      0.86      0.84      1734\n",
      "        PART       0.95      0.76      0.84      5125\n",
      "        PRON       0.83      0.87      0.85      7444\n",
      "       PROPN       0.74      0.42      0.53      5473\n",
      "       PUNCT       1.00      1.00      1.00     29186\n",
      "       SCONJ       0.81      0.90      0.85      2865\n",
      "         SYM       1.00      0.82      0.90        62\n",
      "        VERB       0.89      0.89      0.89     17110\n",
      "           X       0.33      0.01      0.01       134\n",
      "\n",
      "    accuracy                           0.90    153590\n",
      "   macro avg       0.81      0.76      0.77    153590\n",
      "weighted avg       0.90      0.90      0.90    153590\n",
      "\n",
      "HashingVectorizer(analyzer='char', n_features=5000, ngram_range=(1, 5))\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ADJ       0.89      0.89      0.89     15103\n",
      "         ADP       0.98      0.99      0.99     13717\n",
      "         ADV       0.90      0.85      0.87      7783\n",
      "         AUX       0.81      0.97      0.88      1390\n",
      "       CCONJ       0.89      0.97      0.93      5672\n",
      "         DET       0.88      0.72      0.79      4265\n",
      "        INTJ       0.00      0.00      0.00        24\n",
      "        NOUN       0.88      0.95      0.91     36238\n",
      "      NO_TAG       1.00      0.77      0.87       265\n",
      "         NUM       0.85      0.85      0.85      1734\n",
      "        PART       0.93      0.78      0.85      5125\n",
      "        PRON       0.83      0.90      0.86      7444\n",
      "       PROPN       0.79      0.46      0.58      5473\n",
      "       PUNCT       1.00      1.00      1.00     29186\n",
      "       SCONJ       0.81      0.90      0.85      2865\n",
      "         SYM       1.00      0.73      0.84        62\n",
      "        VERB       0.91      0.91      0.91     17110\n",
      "           X       0.32      0.07      0.11       134\n",
      "\n",
      "    accuracy                           0.91    153590\n",
      "   macro avg       0.82      0.76      0.78    153590\n",
      "weighted avg       0.91      0.91      0.91    153590\n",
      "\n",
      "HashingVectorizer(n_features=2000, ngram_range=(1, 5))\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ADJ       0.47      0.25      0.32     15103\n",
      "         ADP       0.89      0.47      0.62     13717\n",
      "         ADV       0.66      0.67      0.66      7783\n",
      "         AUX       0.75      0.94      0.84      1390\n",
      "       CCONJ       0.92      0.18      0.30      5672\n",
      "         DET       0.67      0.50      0.57      4265\n",
      "        INTJ       0.00      0.00      0.00        24\n",
      "        NOUN       0.60      0.58      0.59     36238\n",
      "      NO_TAG       0.00      0.00      0.00       265\n",
      "         NUM       0.51      0.45      0.48      1734\n",
      "        PART       0.90      0.75      0.82      5125\n",
      "        PRON       0.68      0.83      0.75      7444\n",
      "       PROPN       0.37      0.09      0.15      5473\n",
      "       PUNCT       0.48      1.00      0.65     29186\n",
      "       SCONJ       0.76      0.90      0.82      2865\n",
      "         SYM       0.00      0.00      0.00        62\n",
      "        VERB       0.54      0.29      0.38     17110\n",
      "           X       0.00      0.00      0.00       134\n",
      "\n",
      "    accuracy                           0.58    153590\n",
      "   macro avg       0.51      0.44      0.44    153590\n",
      "weighted avg       0.61      0.58      0.55    153590\n",
      "\n",
      "HashingVectorizer(n_features=3000, ngram_range=(1, 5))\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ADJ       0.53      0.27      0.36     15103\n",
      "         ADP       0.92      0.47      0.62     13717\n",
      "         ADV       0.75      0.69      0.72      7783\n",
      "         AUX       0.79      0.74      0.76      1390\n",
      "       CCONJ       0.93      0.18      0.30      5672\n",
      "         DET       0.66      0.62      0.64      4265\n",
      "        INTJ       0.00      0.00      0.00        24\n",
      "        NOUN       0.63      0.60      0.61     36238\n",
      "      NO_TAG       0.00      0.00      0.00       265\n",
      "         NUM       0.59      0.48      0.53      1734\n",
      "        PART       0.90      0.76      0.82      5125\n",
      "        PRON       0.78      0.76      0.77      7444\n",
      "       PROPN       0.44      0.11      0.18      5473\n",
      "       PUNCT       0.46      1.00      0.63     29186\n",
      "       SCONJ       0.73      0.95      0.82      2865\n",
      "         SYM       0.00      0.00      0.00        62\n",
      "        VERB       0.59      0.32      0.41     17110\n",
      "           X       0.00      0.00      0.00       134\n",
      "\n",
      "    accuracy                           0.59    153590\n",
      "   macro avg       0.54      0.44      0.45    153590\n",
      "weighted avg       0.64      0.59      0.57    153590\n",
      "\n",
      "HashingVectorizer(n_features=5000, ngram_range=(1, 5))\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ADJ       0.61      0.30      0.41     15103\n",
      "         ADP       0.92      0.48      0.63     13717\n",
      "         ADV       0.80      0.71      0.75      7783\n",
      "         AUX       0.81      0.86      0.83      1390\n",
      "       CCONJ       0.85      0.20      0.33      5672\n",
      "         DET       0.77      0.54      0.63      4265\n",
      "        INTJ       0.00      0.00      0.00        24\n",
      "        NOUN       0.67      0.62      0.64     36238\n",
      "      NO_TAG       0.00      0.00      0.00       265\n",
      "         NUM       0.63      0.49      0.55      1734\n",
      "        PART       0.93      0.72      0.81      5125\n",
      "        PRON       0.74      0.82      0.77      7444\n",
      "       PROPN       0.51      0.14      0.22      5473\n",
      "       PUNCT       0.45      1.00      0.62     29186\n",
      "       SCONJ       0.76      0.88      0.81      2865\n",
      "         SYM       0.00      0.00      0.00        62\n",
      "        VERB       0.65      0.35      0.46     17110\n",
      "           X       0.00      0.00      0.00       134\n",
      "\n",
      "    accuracy                           0.60    153590\n",
      "   macro avg       0.56      0.45      0.47    153590\n",
      "weighted avg       0.67      0.60      0.59    153590\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vectorizers = [CountVectorizer(ngram_range=(1, 5), analyzer='char'), \n",
    "               TfidfVectorizer(ngram_range=(1, 5), analyzer='char'), \n",
    "               HashingVectorizer(ngram_range=(1, 5), analyzer='char', n_features=1000)] \n",
    "vectorizers_word = [CountVectorizer(ngram_range=(1, 5), analyzer='word'), \n",
    "               TfidfVectorizer(ngram_range=(1, 5), analyzer='word'), \n",
    "               HashingVectorizer(ngram_range=(1, 5), analyzer='word', n_features=1000)] \n",
    "n_features = [2000, 3000, 5000]\n",
    "vectorizers_hash = [HashingVectorizer(ngram_range=(1, 5), analyzer='char', n_features=feat) for feat in n_features]\n",
    "vectorizers_hash_word = [HashingVectorizer(ngram_range=(1, 5), analyzer='word', n_features=feat) for feat in n_features]\n",
    "f1_scores = []\n",
    "accuracy_scores = []\n",
    "\n",
    "for vectorizer in vectorizers + vectorizers_word + vectorizers_hash + vectorizers_hash_word:\n",
    "    X_train = vectorizer.fit_transform(train_tok)\n",
    "    X_test = vectorizer.transform(test_tok)\n",
    "    \n",
    "    lr = LogisticRegression(random_state=0, max_iter=100)\n",
    "    lr.fit(X_train, train_enc_labels)\n",
    "    pred = lr.predict(X_test)\n",
    "    f1 = f1_score(test_enc_labels, pred, average='weighted')\n",
    "    f1_scores.append(f1)\n",
    "    acc = accuracy_score(test_enc_labels, pred)\n",
    "    accuracy_scores.append(acc)\n",
    "    \n",
    "    print(vectorizer)\n",
    "    print(classification_report(test_enc_labels, pred, target_names=le.classes_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Данные в виде таблицы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)  \n",
    "pd.set_option('display.expand_frame_repr', False)\n",
    "pd.set_option('max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Vectorizer</th>\n",
       "      <th>f1_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CountVectorizer(analyzer='char', ngram_range=(1, 5))</td>\n",
       "      <td>0.937951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TfidfVectorizer(analyzer='char', ngram_range=(1, 5))</td>\n",
       "      <td>0.924733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>HashingVectorizer(analyzer='char', n_features=5000, ngram_range=(1, 5))</td>\n",
       "      <td>0.909227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>HashingVectorizer(analyzer='char', n_features=3000, ngram_range=(1, 5))</td>\n",
       "      <td>0.899173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>HashingVectorizer(analyzer='char', n_features=2000, ngram_range=(1, 5))</td>\n",
       "      <td>0.891962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HashingVectorizer(analyzer='char', n_features=1000, ngram_range=(1, 5))</td>\n",
       "      <td>0.864823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CountVectorizer(ngram_range=(1, 5))</td>\n",
       "      <td>0.668761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TfidfVectorizer(ngram_range=(1, 5))</td>\n",
       "      <td>0.646806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>HashingVectorizer(n_features=5000, ngram_range=(1, 5))</td>\n",
       "      <td>0.585157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>HashingVectorizer(n_features=3000, ngram_range=(1, 5))</td>\n",
       "      <td>0.565790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>HashingVectorizer(n_features=2000, ngram_range=(1, 5))</td>\n",
       "      <td>0.548889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>HashingVectorizer(n_features=1000, ngram_range=(1, 5))</td>\n",
       "      <td>0.340380</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                 Vectorizer  f1_score\n",
       "0                      CountVectorizer(analyzer='char', ngram_range=(1, 5))  0.937951\n",
       "1                      TfidfVectorizer(analyzer='char', ngram_range=(1, 5))  0.924733\n",
       "8   HashingVectorizer(analyzer='char', n_features=5000, ngram_range=(1, 5))  0.909227\n",
       "7   HashingVectorizer(analyzer='char', n_features=3000, ngram_range=(1, 5))  0.899173\n",
       "6   HashingVectorizer(analyzer='char', n_features=2000, ngram_range=(1, 5))  0.891962\n",
       "2   HashingVectorizer(analyzer='char', n_features=1000, ngram_range=(1, 5))  0.864823\n",
       "3                                       CountVectorizer(ngram_range=(1, 5))  0.668761\n",
       "4                                       TfidfVectorizer(ngram_range=(1, 5))  0.646806\n",
       "11                   HashingVectorizer(n_features=5000, ngram_range=(1, 5))  0.585157\n",
       "10                   HashingVectorizer(n_features=3000, ngram_range=(1, 5))  0.565790\n",
       "9                    HashingVectorizer(n_features=2000, ngram_range=(1, 5))  0.548889\n",
       "5                    HashingVectorizer(n_features=1000, ngram_range=(1, 5))  0.340380"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_model = pd.DataFrame({'Vectorizer': vectorizers + vectorizers_word + vectorizers_hash + vectorizers_hash_word,\n",
    "                            'f1_score': f1_scores})\n",
    "result_model.sort_values('f1_score', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Vectorizer</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CountVectorizer(analyzer='char', ngram_range=(1, 5))</td>\n",
       "      <td>0.939456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TfidfVectorizer(analyzer='char', ngram_range=(1, 5))</td>\n",
       "      <td>0.927606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>HashingVectorizer(analyzer='char', n_features=5000, ngram_range=(1, 5))</td>\n",
       "      <td>0.912403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>HashingVectorizer(analyzer='char', n_features=3000, ngram_range=(1, 5))</td>\n",
       "      <td>0.902904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>HashingVectorizer(analyzer='char', n_features=2000, ngram_range=(1, 5))</td>\n",
       "      <td>0.895540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HashingVectorizer(analyzer='char', n_features=1000, ngram_range=(1, 5))</td>\n",
       "      <td>0.868644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CountVectorizer(ngram_range=(1, 5))</td>\n",
       "      <td>0.659340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TfidfVectorizer(ngram_range=(1, 5))</td>\n",
       "      <td>0.636402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>HashingVectorizer(n_features=5000, ngram_range=(1, 5))</td>\n",
       "      <td>0.603887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>HashingVectorizer(n_features=3000, ngram_range=(1, 5))</td>\n",
       "      <td>0.589661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>HashingVectorizer(n_features=2000, ngram_range=(1, 5))</td>\n",
       "      <td>0.578215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>HashingVectorizer(n_features=1000, ngram_range=(1, 5))</td>\n",
       "      <td>0.360479</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                 Vectorizer  Accuracy\n",
       "0                      CountVectorizer(analyzer='char', ngram_range=(1, 5))  0.939456\n",
       "1                      TfidfVectorizer(analyzer='char', ngram_range=(1, 5))  0.927606\n",
       "8   HashingVectorizer(analyzer='char', n_features=5000, ngram_range=(1, 5))  0.912403\n",
       "7   HashingVectorizer(analyzer='char', n_features=3000, ngram_range=(1, 5))  0.902904\n",
       "6   HashingVectorizer(analyzer='char', n_features=2000, ngram_range=(1, 5))  0.895540\n",
       "2   HashingVectorizer(analyzer='char', n_features=1000, ngram_range=(1, 5))  0.868644\n",
       "3                                       CountVectorizer(ngram_range=(1, 5))  0.659340\n",
       "4                                       TfidfVectorizer(ngram_range=(1, 5))  0.636402\n",
       "11                   HashingVectorizer(n_features=5000, ngram_range=(1, 5))  0.603887\n",
       "10                   HashingVectorizer(n_features=3000, ngram_range=(1, 5))  0.589661\n",
       "9                    HashingVectorizer(n_features=2000, ngram_range=(1, 5))  0.578215\n",
       "5                    HashingVectorizer(n_features=1000, ngram_range=(1, 5))  0.360479"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_model_acc = pd.DataFrame({'Vectorizer': vectorizers + vectorizers_word + vectorizers_hash + vectorizers_hash_word,\n",
    "                            'Accuracy': accuracy_scores})\n",
    "result_model_acc.sort_values('Accuracy', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задание 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package tagsets to /home/dmitriy/nltk_data...\n",
      "[nltk_data]   Package tagsets is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/dmitriy/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     /home/dmitriy/nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to /home/dmitriy/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('tagsets')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RB: adverb\n",
      "    occasionally unabatingly maddeningly adventurously professedly\n",
      "    stirringly prominently technologically magisterially predominately\n",
      "    swiftly fiscally pitilessly ...\n",
      "NN: noun, common, singular or mass\n",
      "    common-carrier cabbage knuckle-duster Casino afghan shed thermostat\n",
      "    investment slide humour falloff slick wind hyena override subhumanity\n",
      "    machinist ...\n",
      "VB: verb, base form\n",
      "    ask assemble assess assign assume atone attention avoid bake balkanize\n",
      "    bank begin behold believe bend benefit bevel beware bless boil bomb\n",
      "    boost brace break bring broil brush build ...\n"
     ]
    }
   ],
   "source": [
    "nltk.help.upenn_tagset('RB')\n",
    "nltk.help.upenn_tagset('NN')\n",
    "nltk.help.upenn_tagset('VB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "records = load_ne5(os.path.join(data_path, r'Collection5/'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "document = next(records)\n",
    "text = document.text\n",
    "text = re.sub('\\r\\n\\r\\n',' ',text)\n",
    "text = re.sub('\\r\\n',' ',text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "document.text = text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Валентина', 'JJ'),\n",
       " ('Матвиенко', 'NNP'),\n",
       " ('стала', 'NNP'),\n",
       " ('сенатором', 'NNP'),\n",
       " ('Бывший', 'NNP'),\n",
       " ('губернатор', 'NNP'),\n",
       " ('Санкт-Петербурга', 'JJ'),\n",
       " ('Валентина', 'NNP'),\n",
       " ('Матвиенко', 'NNP'),\n",
       " ('вошла', 'NNP'),\n",
       " ('в', 'NNP'),\n",
       " ('состав', 'NNP'),\n",
       " ('Совета', 'NNP'),\n",
       " ('Федерации', 'NNP'),\n",
       " (',', ','),\n",
       " ('сообщается', 'NNP'),\n",
       " ('на', 'NNP'),\n",
       " ('сайте', 'NNP'),\n",
       " ('администрации', 'NNP'),\n",
       " ('Петербурга', 'NNP'),\n",
       " ('.', '.'),\n",
       " ('Матвиенко', 'VB'),\n",
       " ('назначена', 'JJ'),\n",
       " ('по', 'NNP'),\n",
       " ('указу', 'NNP'),\n",
       " ('губернатора', 'NNP'),\n",
       " ('Петербурга', 'NNP'),\n",
       " ('Георгия', 'NNP'),\n",
       " ('Полтавченко', 'NNP'),\n",
       " ('как', 'NNP'),\n",
       " ('представитель', 'NNP'),\n",
       " ('города', 'NNP'),\n",
       " ('от', 'NNP'),\n",
       " ('исполнительной', 'NNP'),\n",
       " ('власти', 'NNP'),\n",
       " ('.', '.'),\n",
       " ('Постановление', 'VB'),\n",
       " ('подписано', 'JJ'),\n",
       " ('в', 'NNP'),\n",
       " ('среду', 'NNP'),\n",
       " (',', ','),\n",
       " ('31', 'CD'),\n",
       " ('августа', 'NN'),\n",
       " (',', ','),\n",
       " ('оно', 'NNP'),\n",
       " ('вступило', 'NNP'),\n",
       " ('в', 'NNP'),\n",
       " ('силу', 'NNP'),\n",
       " ('в', 'NNP'),\n",
       " ('день', 'NNP'),\n",
       " ('подписания', 'NNP'),\n",
       " ('.', '.'),\n",
       " ('Полтавченко', 'VB'),\n",
       " ('подписал', 'JJ'),\n",
       " ('указ', 'NNP'),\n",
       " ('о', 'NNP'),\n",
       " ('назначении', 'NNP'),\n",
       " ('Матвиенко', 'NNP'),\n",
       " ('сенатором', 'NNP'),\n",
       " ('в', 'NNP'),\n",
       " ('день', 'NNP'),\n",
       " ('своего', 'NNP'),\n",
       " ('официального', 'NNP'),\n",
       " ('утверждения', 'NNP'),\n",
       " ('на', 'NNP'),\n",
       " ('посту', 'NNP'),\n",
       " ('губернатора', 'NNP'),\n",
       " ('Санкт-Петербурга', 'NNP'),\n",
       " ('.', '.'),\n",
       " ('До', 'VB'),\n",
       " ('назначения', 'JJ'),\n",
       " ('Матвиенко', 'NNP'),\n",
       " ('в', 'NNP'),\n",
       " ('Совет', 'NNP'),\n",
       " ('Федерации', 'NNP'),\n",
       " ('место', 'NNP'),\n",
       " ('сенатора', 'NNP'),\n",
       " ('от', 'NNP'),\n",
       " ('правительства', 'NNP'),\n",
       " ('Петербурга', 'NNP'),\n",
       " ('в', 'NNP'),\n",
       " ('верхней', 'NNP'),\n",
       " ('палате', 'NNP'),\n",
       " ('парламента', 'NNP'),\n",
       " ('занимал', 'NNP'),\n",
       " ('Владимир', 'NNP'),\n",
       " ('Барканов', 'NNP'),\n",
       " ('.', '.'),\n",
       " ('Представителя', 'VB'),\n",
       " ('от', 'JJ'),\n",
       " ('законодательного', 'NNP'),\n",
       " ('собрания', 'NNP'),\n",
       " ('у', 'NNP'),\n",
       " ('города', 'NNP'),\n",
       " ('в', 'NNP'),\n",
       " ('Совете', 'NNP'),\n",
       " ('Федерации', 'NNP'),\n",
       " ('нет', 'NNP'),\n",
       " ('.', '.'),\n",
       " ('Валентина', 'VB'),\n",
       " ('Матвиенко', 'JJ'),\n",
       " ('оставила', 'NNP'),\n",
       " ('пост', 'NNP'),\n",
       " ('губернатора', 'NNP'),\n",
       " ('Петербурга', 'NNP'),\n",
       " (',', ','),\n",
       " ('который', 'NNP'),\n",
       " ('она', 'NNP'),\n",
       " ('занимала', 'NNP'),\n",
       " ('на', 'NNP'),\n",
       " ('протяжении', 'NNP'),\n",
       " ('восьми', 'NNP'),\n",
       " ('лет', 'NNP'),\n",
       " (',', ','),\n",
       " ('ради', 'NNP'),\n",
       " ('должности', 'NNP'),\n",
       " ('спикера', 'NNP'),\n",
       " ('Совета', 'NNP'),\n",
       " ('Федерации', 'NNP'),\n",
       " (',', ','),\n",
       " ('которая', 'NNP'),\n",
       " ('остается', 'NNP'),\n",
       " ('вакантной', 'NNP'),\n",
       " ('после', 'NNP'),\n",
       " ('отзыва', 'NNP'),\n",
       " ('из', 'NNP'),\n",
       " ('верхней', 'NNP'),\n",
       " ('палаты', 'NNP'),\n",
       " ('парламента', 'NNP'),\n",
       " ('Сергея', 'NNP'),\n",
       " ('Миронова', 'NNP'),\n",
       " ('в', 'NNP'),\n",
       " ('мае', 'NNP'),\n",
       " ('2011', 'CD'),\n",
       " ('года', 'NNP'),\n",
       " ('.', '.'),\n",
       " ('Для', 'NN'),\n",
       " ('того', 'NN'),\n",
       " (',', ','),\n",
       " ('чтобы', 'NNP'),\n",
       " ('войти', 'NNP'),\n",
       " ('в', 'NNP'),\n",
       " ('состав', 'NNP'),\n",
       " ('СФ', 'NNP'),\n",
       " (',', ','),\n",
       " ('Матвиенко', 'NNP'),\n",
       " ('получила', 'NNP'),\n",
       " ('статус', 'NNP'),\n",
       " ('депутата', 'NNP'),\n",
       " (',', ','),\n",
       " ('победив', 'NNP'),\n",
       " ('на', 'NNP'),\n",
       " ('выборах', 'NNP'),\n",
       " ('в', 'NNP'),\n",
       " ('районе', 'NNP'),\n",
       " ('Красненькая', 'NNP'),\n",
       " ('речка', 'NNP'),\n",
       " ('.', '.'),\n",
       " ('Выдвинуть', 'VB'),\n",
       " ('Матвиенко', 'JJ'),\n",
       " ('на', 'NNP'),\n",
       " ('пост', 'NNP'),\n",
       " ('спикера', 'NNP'),\n",
       " ('Совета', 'NNP'),\n",
       " ('Федерации', 'NNP'),\n",
       " ('предложила', 'NNP'),\n",
       " ('группа', 'NNP'),\n",
       " ('губернаторов', 'NNP'),\n",
       " ('спустя', 'NNP'),\n",
       " ('месяц', 'NNP'),\n",
       " ('после', 'NNP'),\n",
       " ('отставки', 'NNP'),\n",
       " ('Миронова', 'NNP'),\n",
       " (',', ','),\n",
       " ('президент', 'NNP'),\n",
       " ('Дмитрий', 'NNP'),\n",
       " ('Медведев', 'NNP'),\n",
       " ('идею', 'NNP'),\n",
       " ('поддержал', 'NNP'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.pos_tag(nltk.word_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('Бывший', 'PERSON'),\n",
       " ('Валентина', 'PERSON'),\n",
       " ('Валентина Матвиенко', 'PERSON'),\n",
       " ('Георгия Полтавченко', 'PERSON'),\n",
       " ('Дмитрий Медведев', 'PERSON'),\n",
       " ('Матвиенко', 'ORGANIZATION'),\n",
       " ('Матвиенко', 'PERSON'),\n",
       " ('Сергея Миронова', 'PERSON'),\n",
       " ('Совета Федерации', 'PERSON'),\n",
       " ('Совете Федерации', 'PERSON')}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{(' '.join(c[0] for c in chunk), chunk.label() ) for chunk in nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(text))) if hasattr(chunk, 'label') }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Ne5Span(\n",
       "     index='T1',\n",
       "     type='PER',\n",
       "     start=0,\n",
       "     stop=19,\n",
       "     text='Валентина Матвиенко'\n",
       " ),\n",
       " Ne5Span(\n",
       "     index='T2',\n",
       "     type='LOC',\n",
       "     start=57,\n",
       "     stop=73,\n",
       "     text='Санкт-Петербурга'\n",
       " ),\n",
       " Ne5Span(\n",
       "     index='T3',\n",
       "     type='PER',\n",
       "     start=74,\n",
       "     stop=93,\n",
       "     text='Валентина Матвиенко'\n",
       " ),\n",
       " Ne5Span(\n",
       "     index='T4',\n",
       "     type='ORG',\n",
       "     start=109,\n",
       "     stop=125,\n",
       "     text='Совета Федерации'\n",
       " ),\n",
       " Ne5Span(\n",
       "     index='T5',\n",
       "     type='LOC',\n",
       "     start=161,\n",
       "     stop=171,\n",
       "     text='Петербурга'\n",
       " ),\n",
       " Ne5Span(\n",
       "     index='T6',\n",
       "     type='PER',\n",
       "     start=176,\n",
       "     stop=185,\n",
       "     text='Матвиенко'\n",
       " ),\n",
       " Ne5Span(\n",
       "     index='T7',\n",
       "     type='LOC',\n",
       "     start=217,\n",
       "     stop=227,\n",
       "     text='Петербурга'\n",
       " ),\n",
       " Ne5Span(\n",
       "     index='T8',\n",
       "     type='PER',\n",
       "     start=228,\n",
       "     stop=247,\n",
       "     text='Георгия Полтавченко'\n",
       " ),\n",
       " Ne5Span(\n",
       "     index='T9',\n",
       "     type='PER',\n",
       "     start=386,\n",
       "     stop=397,\n",
       "     text='Полтавченко'\n",
       " ),\n",
       " Ne5Span(\n",
       "     index='T10',\n",
       "     type='PER',\n",
       "     start=425,\n",
       "     stop=434,\n",
       "     text='Матвиенко'\n",
       " ),\n",
       " Ne5Span(\n",
       "     index='T11',\n",
       "     type='LOC',\n",
       "     start=505,\n",
       "     stop=521,\n",
       "     text='Санкт-Петербурга'\n",
       " ),\n",
       " Ne5Span(\n",
       "     index='T12',\n",
       "     type='PER',\n",
       "     start=540,\n",
       "     stop=549,\n",
       "     text='Матвиенко'\n",
       " ),\n",
       " Ne5Span(\n",
       "     index='T13',\n",
       "     type='ORG',\n",
       "     start=552,\n",
       "     stop=567,\n",
       "     text='Совет Федерации'\n",
       " ),\n",
       " Ne5Span(\n",
       "     index='T14',\n",
       "     type='LOC',\n",
       "     start=600,\n",
       "     stop=610,\n",
       "     text='Петербурга'\n",
       " ),\n",
       " Ne5Span(\n",
       "     index='T15',\n",
       "     type='PER',\n",
       "     start=647,\n",
       "     stop=664,\n",
       "     text='Владимир Барканов'\n",
       " ),\n",
       " Ne5Span(\n",
       "     index='T16',\n",
       "     type='ORG',\n",
       "     start=720,\n",
       "     stop=736,\n",
       "     text='Совете Федерации'\n",
       " ),\n",
       " Ne5Span(\n",
       "     index='T17',\n",
       "     type='PER',\n",
       "     start=745,\n",
       "     stop=764,\n",
       "     text='Валентина Матвиенко'\n",
       " ),\n",
       " Ne5Span(\n",
       "     index='T18',\n",
       "     type='LOC',\n",
       "     start=791,\n",
       "     stop=801,\n",
       "     text='Петербурга'\n",
       " ),\n",
       " Ne5Span(\n",
       "     index='T19',\n",
       "     type='ORG',\n",
       "     start=873,\n",
       "     stop=889,\n",
       "     text='Совета Федерации'\n",
       " ),\n",
       " Ne5Span(\n",
       "     index='T20',\n",
       "     type='PER',\n",
       "     start=960,\n",
       "     stop=975,\n",
       "     text='Сергея Миронова'\n",
       " ),\n",
       " Ne5Span(\n",
       "     index='T21',\n",
       "     type='ORG',\n",
       "     start=1024,\n",
       "     stop=1026,\n",
       "     text='СФ'\n",
       " ),\n",
       " Ne5Span(\n",
       "     index='T22',\n",
       "     type='PER',\n",
       "     start=1028,\n",
       "     stop=1037,\n",
       "     text='Матвиенко'\n",
       " ),\n",
       " Ne5Span(\n",
       "     index='T23',\n",
       "     type='LOC',\n",
       "     start=1092,\n",
       "     stop=1109,\n",
       "     text='Красненькая речка'\n",
       " ),\n",
       " Ne5Span(\n",
       "     index='T24',\n",
       "     type='PER',\n",
       "     start=1121,\n",
       "     stop=1130,\n",
       "     text='Матвиенко'\n",
       " ),\n",
       " Ne5Span(\n",
       "     index='T25',\n",
       "     type='ORG',\n",
       "     start=1147,\n",
       "     stop=1163,\n",
       "     text='Совета Федерации'\n",
       " ),\n",
       " Ne5Span(\n",
       "     index='T26',\n",
       "     type='PER',\n",
       "     start=1223,\n",
       "     stop=1231,\n",
       "     text='Миронова'\n",
       " ),\n",
       " Ne5Span(\n",
       "     index='T27',\n",
       "     type='PER',\n",
       "     start=1243,\n",
       "     stop=1259,\n",
       "     text='Дмитрий Медведев'\n",
       " )]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document.spans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Встречаются ошибки."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SpaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"ru_core_news_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "ny_bb = text\n",
    "article = nlp(ny_bb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Валентина Матвиенко\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PER</span>\n",
       "</mark>\n",
       " стала сенатором Бывший губернатор \n",
       "<mark class=\"entity\" style=\"background: #ff9561; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Санкт-Петербурга\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">LOC</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Валентина Матвиенко\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PER</span>\n",
       "</mark>\n",
       " вошла в состав \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Совета Федерации\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       ", сообщается на сайте администрации \n",
       "<mark class=\"entity\" style=\"background: #ff9561; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Петербурга\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">LOC</span>\n",
       "</mark>\n",
       ". \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Матвиенко\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PER</span>\n",
       "</mark>\n",
       " назначена по указу губернатора \n",
       "<mark class=\"entity\" style=\"background: #ff9561; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Петербурга\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">LOC</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Георгия Полтавченко\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PER</span>\n",
       "</mark>\n",
       " как представитель города от исполнительной власти. Постановление подписано в среду, 31 августа, оно вступило в силу в день подписания. \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Полтавченко\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PER</span>\n",
       "</mark>\n",
       " подписал указ о назначении \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Матвиенко\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PER</span>\n",
       "</mark>\n",
       " сенатором в день своего официального утверждения на посту губернатора \n",
       "<mark class=\"entity\" style=\"background: #ff9561; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Санкт-Петербурга\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">LOC</span>\n",
       "</mark>\n",
       ". До назначения \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Матвиенко\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PER</span>\n",
       "</mark>\n",
       " в \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Совет Федерации\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " место сенатора от правительства \n",
       "<mark class=\"entity\" style=\"background: #ff9561; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Петербурга\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">LOC</span>\n",
       "</mark>\n",
       " в верхней палате парламента занимал \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Владимир Барканов\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PER</span>\n",
       "</mark>\n",
       ". Представителя от законодательного собрания у города в \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Совете Федерации\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " нет. \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Валентина Матвиенко\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PER</span>\n",
       "</mark>\n",
       " оставила пост губернатора \n",
       "<mark class=\"entity\" style=\"background: #ff9561; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Петербурга\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">LOC</span>\n",
       "</mark>\n",
       ", который она занимала на протяжении восьми лет, ради должности спикера \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Совета Федерации\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       ", которая остается вакантной после отзыва из верхней палаты парламента \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Сергея Миронова\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PER</span>\n",
       "</mark>\n",
       " в мае 2011 года. Для того, чтобы войти в состав \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    СФ\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       ", \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Матвиенко\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PER</span>\n",
       "</mark>\n",
       " получила статус депутата, победив на выборах в районе \n",
       "<mark class=\"entity\" style=\"background: #ff9561; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Красненькая речка\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">LOC</span>\n",
       "</mark>\n",
       ". Выдвинуть \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Матвиенко\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PER</span>\n",
       "</mark>\n",
       " на пост спикера \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Совета Федерации\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " предложила группа губернаторов спустя месяц после отставки \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Миронова\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PER</span>\n",
       "</mark>\n",
       ", президент \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Дмитрий Медведев\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PER</span>\n",
       "</mark>\n",
       " идею поддержал. </div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "displacy.render(article, jupyter=True, style='ent')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Работает без ошибок"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Валентина PROPN nsubj\n",
      "Матвиенко PROPN appos\n",
      "стала VERB ROOT\n",
      "сенатором NOUN xcomp\n",
      "Бывший ADJ amod\n",
      "губернатор NOUN nsubj\n",
      "Санкт PROPN nmod\n",
      "- PROPN nmod\n",
      "Петербурга PROPN nmod\n",
      "Валентина PROPN appos\n",
      "Матвиенко PROPN flat:name\n",
      "вошла VERB conj\n",
      "в ADP case\n",
      "состав NOUN obl\n",
      "Совета PROPN nmod\n",
      "Федерации PROPN nmod\n",
      ", PUNCT punct\n",
      "сообщается VERB parataxis\n",
      "на ADP case\n",
      "сайте NOUN obl\n",
      "администрации NOUN nmod\n",
      "Петербурга PROPN nmod\n",
      ". PUNCT punct\n",
      "Матвиенко PROPN nsubj:pass\n",
      "назначена VERB ROOT\n",
      "по ADP case\n",
      "указу NOUN obl\n",
      "губернатора NOUN nmod\n",
      "Петербурга PROPN nmod\n",
      "Георгия PROPN appos\n",
      "Полтавченко PROPN flat:name\n",
      "как SCONJ case\n",
      "представитель NOUN obl\n",
      "города NOUN nmod\n",
      "от ADP case\n",
      "исполнительной ADJ amod\n",
      "власти NOUN nmod\n",
      ". PUNCT punct\n",
      "Постановление NOUN nsubj:pass\n",
      "подписано VERB ROOT\n",
      "в ADP case\n",
      "среду NOUN obl\n",
      ", PUNCT punct\n",
      "31 ADJ nmod\n",
      "августа NOUN flat\n",
      ", PUNCT punct\n",
      "оно PRON nsubj\n",
      "вступило VERB conj\n",
      "в ADP case\n",
      "силу NOUN obl\n",
      "в ADP case\n",
      "день NOUN obl\n",
      "подписания NOUN nmod\n",
      ". PUNCT punct\n",
      "Полтавченко PROPN nsubj\n",
      "подписал VERB ROOT\n",
      "указ NOUN obj\n",
      "о ADP case\n",
      "назначении NOUN nmod\n",
      "Матвиенко PROPN nmod\n",
      "сенатором NOUN nmod\n",
      "в ADP case\n",
      "день NOUN nmod\n",
      "своего DET det\n",
      "официального ADJ amod\n",
      "утверждения NOUN nmod\n",
      "на ADP case\n",
      "посту NOUN nmod\n",
      "губернатора NOUN nmod\n",
      "Санкт PROPN nmod\n",
      "- PROPN nmod\n",
      "Петербурга PROPN nmod\n",
      ". PUNCT punct\n",
      "До ADP case\n",
      "назначения NOUN obl\n",
      "Матвиенко PROPN nmod\n",
      "в ADP case\n",
      "Совет PROPN nmod\n",
      "Федерации PROPN nmod\n",
      "место NOUN obj\n",
      "сенатора NOUN nmod\n",
      "от ADP case\n",
      "правительства NOUN nmod\n",
      "Петербурга PROPN nmod\n",
      "в ADP case\n",
      "верхней ADJ amod\n",
      "палате NOUN nmod\n",
      "парламента NOUN nmod\n",
      "занимал VERB ROOT\n",
      "Владимир PROPN nsubj\n",
      "Барканов PROPN flat:name\n",
      ". PUNCT punct\n",
      "Представителя NOUN nsubj\n",
      "от ADP case\n",
      "законодательного ADJ amod\n",
      "собрания NOUN nmod\n",
      "у ADP case\n",
      "города NOUN obl\n",
      "в ADP case\n",
      "Совете PROPN nmod\n",
      "Федерации PROPN nmod\n",
      "нет VERB ROOT\n",
      ". PUNCT punct\n",
      "Валентина PROPN nsubj\n",
      "Матвиенко PROPN appos\n",
      "оставила VERB ROOT\n",
      "пост NOUN obj\n",
      "губернатора NOUN nmod\n",
      "Петербурга PROPN nmod\n",
      ", PUNCT punct\n",
      "который PRON obj\n",
      "она PRON nsubj\n",
      "занимала VERB acl:relcl\n",
      "на ADP case\n",
      "протяжении NOUN obl\n",
      "восьми NUM nummod\n",
      "лет NOUN nmod\n",
      ", PUNCT punct\n",
      "ради ADP case\n",
      "должности NOUN obl\n",
      "спикера NOUN nmod\n",
      "Совета PROPN nmod\n",
      "Федерации PROPN nmod\n",
      ", PUNCT punct\n",
      "которая PRON nsubj\n",
      "остается VERB acl:relcl\n",
      "вакантной ADJ xcomp\n",
      "после ADP case\n",
      "отзыва NOUN obl\n",
      "из ADP case\n",
      "верхней ADJ amod\n",
      "палаты NOUN nmod\n",
      "парламента NOUN nmod\n",
      "Сергея PROPN nmod\n",
      "Миронова PROPN flat:name\n",
      "в ADP case\n",
      "мае NOUN obl\n",
      "2011 ADJ amod\n",
      "года NOUN nmod\n",
      ". PUNCT punct\n",
      "Для ADP case\n",
      "того PRON obl\n",
      ", PUNCT punct\n",
      "чтобы SCONJ mark\n",
      "войти VERB advcl\n",
      "в ADP case\n",
      "состав NOUN obl\n",
      "СФ PROPN nmod\n",
      ", PUNCT punct\n",
      "Матвиенко PROPN nsubj\n",
      "получила VERB ROOT\n",
      "статус NOUN obj\n",
      "депутата NOUN nmod\n",
      ", PUNCT punct\n",
      "победив VERB advcl\n",
      "на ADP case\n",
      "выборах NOUN obl\n",
      "в ADP case\n",
      "районе NOUN nmod\n",
      "Красненькая ADJ amod\n",
      "речка NOUN appos\n",
      ". PUNCT punct\n",
      "Выдвинуть VERB xcomp\n",
      "Матвиенко PROPN obj\n",
      "на ADP case\n",
      "пост NOUN obl\n",
      "спикера NOUN nmod\n",
      "Совета PROPN nmod\n",
      "Федерации PROPN nmod\n",
      "предложила VERB ROOT\n",
      "группа NOUN nsubj\n",
      "губернаторов NOUN nmod\n",
      "спустя ADP case\n",
      "месяц NOUN obl\n",
      "после ADP case\n",
      "отставки NOUN nmod\n",
      "Миронова PROPN nmod\n",
      ", PUNCT punct\n",
      "президент NOUN nsubj\n",
      "Дмитрий PROPN appos\n",
      "Медведев PROPN flat:name\n",
      "идею NOUN obj\n",
      "поддержал VERB conj\n",
      ". PUNCT punct\n"
     ]
    }
   ],
   "source": [
    "for token in article:\n",
    "    print(token.text, token.pos_, token.dep_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Свой NER."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_classification_report(y_test_true, y_test_pred):\n",
    "    print(classification_report(y_test_true, y_test_pred))\n",
    "\n",
    "    print('CONFUSION MATRIX\\n')\n",
    "    print(pd.crosstab(y_test_true, y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "records = load_ne5(os.path.join(data_path, r'Collection5/'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_docs = []\n",
    "for ix, rec in enumerate(records):\n",
    "    words = []\n",
    "    for token in tokenize(rec.text):\n",
    "        type_ent = 'OUT'\n",
    "        for ent in rec.spans:\n",
    "            if (token.start >= ent.start) and (token.stop <= ent.stop):\n",
    "                type_ent = ent.type\n",
    "                break\n",
    "        words.append([token.text, type_ent])\n",
    "    words_docs.extend(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_words = pd.DataFrame(words_docs, columns=['word', 'tag'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tag\n",
       "OUT         219214\n",
       "PER          21200\n",
       "ORG          13651\n",
       "LOC           4568\n",
       "GEOPOLIT      4356\n",
       "MEDIA         2482\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_words['tag'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Валентина</td>\n",
       "      <td>PER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Матвиенко</td>\n",
       "      <td>PER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>стала</td>\n",
       "      <td>OUT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>сенатором</td>\n",
       "      <td>OUT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Бывший</td>\n",
       "      <td>OUT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        word  tag\n",
       "0  Валентина  PER\n",
       "1  Матвиенко  PER\n",
       "2      стала  OUT\n",
       "3  сенатором  OUT\n",
       "4     Бывший  OUT"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_words.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, valid_x, train_y, valid_y = model_selection.train_test_split(df_words['word'], df_words['tag'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Кодировка целевой переменной."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = preprocessing.LabelEncoder()\n",
    "train_y = encoder.fit_transform(train_y)\n",
    "valid_y = encoder.fit_transform(valid_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.apply(len).max(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-17 00:39:06.790849: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1886] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 1636 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1650, pci bus id: 0000:03:00.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "train_data = tf.data.Dataset.from_tensor_slices((train_x, train_y))\n",
    "valid_data = tf.data.Dataset.from_tensor_slices((valid_x, valid_y))\n",
    "train_data = train_data.batch(16)\n",
    "valid_data = valid_data.batch(16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "train_data = train_data.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "valid_data = valid_data.cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_standardization(input_data):\n",
    "    return input_data\n",
    "\n",
    "\n",
    "vocab_size = 30000\n",
    "seq_len = 10\n",
    "vectorize_layer = TextVectorization(  \n",
    "                            standardize=custom_standardization,\n",
    "                            max_tokens=vocab_size,\n",
    "                            output_mode='int',\n",
    "                            #ngrams=(1, 3),\n",
    "                            output_sequence_length=seq_len)\n",
    "text_data = train_data.map(lambda x, y: x)\n",
    "vectorize_layer.adapt(text_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 64\n",
    "\n",
    "class modelNER(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(modelNER, self).__init__()\n",
    "        self.emb = Embedding(vocab_size, embedding_dim)\n",
    "        self.gPool = GlobalMaxPooling1D()\n",
    "        self.fc1 = Dense(300, activation='relu')\n",
    "        self.fc2 = Dense(50, activation='relu')\n",
    "        self.fc3 = Dense(6, activation='softmax') # [OUT, PER, ORG, LOC, GEOPOLIT, MEDIA]\n",
    "\n",
    "    def call(self, x):\n",
    "        x = vectorize_layer(x)\n",
    "        x = self.emb(x)\n",
    "        pool_x = self.gPool(x)\n",
    "        \n",
    "        fc_x = self.fc1(pool_x)\n",
    "        fc_x = self.fc2(fc_x)\n",
    "        \n",
    "        concat_x = tf.concat([pool_x, fc_x], axis=1)\n",
    "        prob = self.fc3(concat_x)\n",
    "        return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "mmodel = modelNER()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "mmodel.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-17 00:39:24.486328: I tensorflow/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2023-12-17 00:39:26.115258: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7f869009e040 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2023-12-17 00:39:26.115305: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce GTX 1650, Compute Capability 7.5\n",
      "2023-12-17 00:39:26.131091: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2023-12-17 00:39:26.844675: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:442] Loaded cuDNN version 8902\n",
      "2023-12-17 00:39:26.956309: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12444/12444 [==============================] - 103s 8ms/step - loss: 0.2941 - accuracy: 0.9145 - val_loss: 0.2130 - val_accuracy: 0.9378\n",
      "Epoch 2/3\n",
      "12444/12444 [==============================] - 97s 8ms/step - loss: 0.1256 - accuracy: 0.9630 - val_loss: 0.4198 - val_accuracy: 0.8935\n",
      "Epoch 3/3\n",
      "12444/12444 [==============================] - 99s 8ms/step - loss: 0.1098 - accuracy: 0.9658 - val_loss: 0.3140 - val_accuracy: 0.8934\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7f875d6e1de0>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mmodel.fit( train_data,\n",
    "            validation_data=valid_data,\n",
    "            epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2074/2074 [==============================] - 5s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "pred_y = mmodel.predict(valid_x)\n",
    "y_pred_classes = np.argmax(pred_y,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9005803397253274"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1 = f1_score(valid_y, y_pred_classes, average= \"weighted\")\n",
    "f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: ['GEOPOLIT' 'LOC' 'MEDIA' 'ORG' 'OUT' 'PER']\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.89      0.90      1082\n",
      "           1       0.86      0.78      0.82      1144\n",
      "           2       0.93      0.75      0.83       643\n",
      "           3       0.86      0.57      0.68      3463\n",
      "           4       0.97      0.92      0.94     54762\n",
      "           5       0.49      0.88      0.63      5274\n",
      "\n",
      "    accuracy                           0.89     66368\n",
      "   macro avg       0.83      0.80      0.80     66368\n",
      "weighted avg       0.92      0.89      0.90     66368\n",
      "\n",
      "CONFUSION MATRIX\n",
      "\n",
      "col_0    0    1    2     3      4     5\n",
      "row_0                                  \n",
      "0      963   23    0    33      8    55\n",
      "1       12  893    4    26     31   178\n",
      "2        5    9  485    26     56    62\n",
      "3       68   42   20  1961    985   387\n",
      "4       16   74   12   239  50331  4090\n",
      "5        1    3    0     3    606  4661\n"
     ]
    }
   ],
   "source": [
    "print(f\"Classes: {encoder.classes_}\\r\\n\")\n",
    "\n",
    "get_classification_report(valid_y, y_pred_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучение на биграммах и триграммах."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_standardization(input_data):\n",
    "    return input_data\n",
    "\n",
    "\n",
    "vocab_size = 30000\n",
    "seq_len = 10\n",
    "vectorize_layer = TextVectorization( \n",
    "                            standardize=custom_standardization,\n",
    "                            max_tokens=vocab_size,\n",
    "                            output_mode='int',\n",
    "                            ngrams=(1, 3),\n",
    "                            output_sequence_length=seq_len)\n",
    "text_data = train_data.map(lambda x, y: x)\n",
    "vectorize_layer.adapt(text_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "mmodel = modelNER()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "mmodel.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12444/12444 [==============================] - 100s 8ms/step - loss: 0.2970 - accuracy: 0.9135 - val_loss: 0.2135 - val_accuracy: 0.9380\n",
      "Epoch 2/3\n",
      "12444/12444 [==============================] - 90s 7ms/step - loss: 0.1267 - accuracy: 0.9627 - val_loss: 0.2898 - val_accuracy: 0.8820\n",
      "Epoch 3/3\n",
      "12444/12444 [==============================] - 90s 7ms/step - loss: 0.1106 - accuracy: 0.9656 - val_loss: 0.3476 - val_accuracy: 0.8829\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7f875b189330>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mmodel.fit(train_data,\n",
    "            validation_data=valid_data,\n",
    "            epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2074/2074 [==============================] - 5s 3ms/step\n"
     ]
    }
   ],
   "source": [
    "pred_y = mmodel.predict(valid_x)\n",
    "y_pred_classes = np.argmax(pred_y,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.906345489697992"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1 = f1_score(valid_y, y_pred_classes, average= \"weighted\")\n",
    "f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: ['GEOPOLIT' 'LOC' 'MEDIA' 'ORG' 'OUT' 'PER']\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.89      0.90      1082\n",
      "           1       0.16      0.93      0.28      1144\n",
      "           2       0.94      0.76      0.84       643\n",
      "           3       0.86      0.56      0.68      3463\n",
      "           4       0.97      0.92      0.94     54762\n",
      "           5       0.98      0.71      0.82      5274\n",
      "\n",
      "    accuracy                           0.88     66368\n",
      "   macro avg       0.80      0.80      0.74     66368\n",
      "weighted avg       0.95      0.88      0.91     66368\n",
      "\n",
      "CONFUSION MATRIX\n",
      "\n",
      "col_0    0     1    2     3      4     5\n",
      "row_0                                   \n",
      "0      962    76    0    36      8     0\n",
      "1       18  1069    0    24     32     1\n",
      "2        5    69  490    20     57     2\n",
      "3       66   408   22  1955    993    19\n",
      "4       14  4075   12   223  50393    45\n",
      "5        1   884    0     4    660  3725\n"
     ]
    }
   ],
   "source": [
    "print(f\"Classes: {encoder.classes_}\\r\\n\")\n",
    "get_classification_report(valid_y, y_pred_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Результат лучше у сети, которая обучалась на N-граммах"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
