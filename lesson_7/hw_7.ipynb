{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Берем отызывы за лето (из архива с материалами или предыдущего занятия)\n",
    "\n",
    "1. Учим conv сеть для классификации\n",
    "2. Рассмотреть 2-а варианта сеточек \n",
    "3. Инициализировать tf.keras.layers.Embedding предобученными векторами взять к примеру с https://rusvectores.org/ru/\n",
    "4. Инициализировать слой tf.keras.layers.Embedding по умолчанию (ну то есть вам ничего не делать с весами)\n",
    " \t\t\t\t\n",
    "Сравнить две архитектуры с предобученными весами и когда tf.keras.layers.Embedding обучается сразу со всей сеточкой, что получилось лучше"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/dmitriy/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "2023-12-25 19:17:43.954235: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-12-25 19:17:44.118180: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-12-25 19:17:44.120040: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-12-25 19:17:44.986999: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "# Импорт библиотек\n",
    "import gensim\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from string import punctuation\n",
    "from stop_words import get_stop_words\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download(\"punkt\")\n",
    "from nltk.probability import FreqDist\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Activation, Input, Embedding, Conv1D, GlobalMaxPool1D\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.callbacks import TensorBoard \n",
    "from keras.losses import SparseCategoricalCrossentropy\n",
    "from keras.callbacks import EarlyStopping  \n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = r'/home/dmitriy/Downloads/ai_nlp_data/hw_7/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "\n",
    "\n",
    "def get_f1(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    f1_val = 2*(precision*recall)/(precision+recall+K.epsilon())\n",
    "    return f1_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(os.path.join(data_path, r'отзывы за лето.xls'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_test = train_test_split(df, test_size=0.33, random_state=42)\n",
    "df_train = df_train.reset_index(drop=True)\n",
    "df_test = df_test.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Предобработка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sw = set(get_stop_words(\"ru\"))\n",
    "exclude = set(punctuation)\n",
    "morpher = MorphAnalyzer()\n",
    "\n",
    "\n",
    "def preprocess_text(txt):\n",
    "    txt = str(txt)\n",
    "    txt = \"\".join(c for c in txt if c not in exclude)\n",
    "    txt = txt.lower()\n",
    "    txt = re.sub(\"\\sне\", \"не\", txt)\n",
    "    txt = [morpher.parse(word)[0].normal_form for word in txt.split() if word not in sw]\n",
    "    return \" \".join(txt)\n",
    "\n",
    "\n",
    "df_train['Content'] = df_train['Content'].apply(preprocess_text)\n",
    "df_test['Content'] = df_test['Content'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_corpus = \" \".join(df_train[\"Content\"])\n",
    "train_corpus = train_corpus.lower()\n",
    "tokens = word_tokenize(train_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_filtered = [word for word in tokens if word.isalnum()]\n",
    "dist = FreqDist(tokens_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words = 200\n",
    "tokens_filtered_top = [pair[0] for pair in dist.most_common(max_words-1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = {v: k for k, v in dict(enumerate(tokens_filtered_top, 1)).items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 40\n",
    "\n",
    "\n",
    "def text_to_sequence(text, maxlen):\n",
    "    result = []\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    tokens_filtered = [word for word in tokens if word.isalnum()]\n",
    "    for word in tokens_filtered:\n",
    "        if word in vocabulary:\n",
    "            result.append(vocabulary[word])\n",
    "    padding = [0]*(maxlen-len(result))\n",
    "    return padding + result[-maxlen:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.asarray([text_to_sequence(text, max_len) for text in df_train[\"Content\"]], dtype=np.int32)\n",
    "x_test = np.asarray([text_to_sequence(text, max_len) for text in df_test[\"Content\"]], dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "train_enc_labels = le.fit_transform(df_train['Rating']) \n",
    "test_enc_labels = le.transform(df_test['Rating'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 5\n",
    "y_train = tf.keras.utils.to_categorical(train_enc_labels, num_classes=num_classes)\n",
    "y_test = tf.keras.utils.to_categorical(test_enc_labels, num_classes=num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras CONV модель с Embedding слоем по умолчанию"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=max_words, output_dim=128, input_length=max_len)) \n",
    "                    #inputdim -- размер словаря, outputdim -- длина вектора, input_length -- длина входной последовательности\n",
    "                    #на вход: (батч, inputlen), на выходе: (батч, inputlen, outputdim)\n",
    "model.add(Conv1D(128, 3))\n",
    "                   #128 -- длина 1D-фильтра, шаг -- 3\n",
    "                   #на выходе ([128/3], 128)\n",
    "model.add(Activation(\"relu\"))\n",
    "                    #применяем функцию активации к выходу предыдущего слоя\n",
    "                    #на выходе ([128/3], 128)\n",
    "model.add(GlobalMaxPool1D())\n",
    "                    #в каждой свертке оставляет максимальный элемент\n",
    "                    #на выходе ([128/3], 1)\n",
    "model.add(Dense(10))\n",
    "               #10-количество выходов\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(Dense(num_classes))\n",
    "                #num_classes = 5 -- количество выходов\n",
    "model.add(Activation('softmax'))\n",
    "                #преобразуем вектор в рапределение вероятностей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=[get_f1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "25/25 [==============================] - 2s 39ms/step - loss: 1.3791 - get_f1: 0.0664 - val_loss: 1.0514 - val_get_f1: 0.7153\n",
      "Epoch 2/20\n",
      "25/25 [==============================] - 1s 33ms/step - loss: 0.9220 - get_f1: 0.7253 - val_loss: 0.8715 - val_get_f1: 0.7433\n",
      "Epoch 3/20\n",
      "25/25 [==============================] - 1s 33ms/step - loss: 0.7711 - get_f1: 0.7645 - val_loss: 0.7735 - val_get_f1: 0.7527\n",
      "Epoch 4/20\n",
      "25/25 [==============================] - 1s 31ms/step - loss: 0.7156 - get_f1: 0.7690 - val_loss: 0.7477 - val_get_f1: 0.7525\n",
      "Epoch 5/20\n",
      "25/25 [==============================] - 1s 31ms/step - loss: 0.6957 - get_f1: 0.7702 - val_loss: 0.7350 - val_get_f1: 0.7572\n",
      "Epoch 6/20\n",
      "25/25 [==============================] - 1s 32ms/step - loss: 0.6807 - get_f1: 0.7770 - val_loss: 0.7241 - val_get_f1: 0.7576\n",
      "Epoch 7/20\n",
      "25/25 [==============================] - 1s 32ms/step - loss: 0.6702 - get_f1: 0.7792 - val_loss: 0.7228 - val_get_f1: 0.7585\n",
      "Epoch 8/20\n",
      "25/25 [==============================] - 1s 33ms/step - loss: 0.6605 - get_f1: 0.7821 - val_loss: 0.7144 - val_get_f1: 0.7608\n",
      "Epoch 9/20\n",
      "25/25 [==============================] - 1s 32ms/step - loss: 0.6520 - get_f1: 0.7836 - val_loss: 0.7095 - val_get_f1: 0.7600\n",
      "Epoch 10/20\n",
      "25/25 [==============================] - 1s 32ms/step - loss: 0.6441 - get_f1: 0.7868 - val_loss: 0.7087 - val_get_f1: 0.7600\n",
      "Epoch 11/20\n",
      "25/25 [==============================] - 1s 32ms/step - loss: 0.6368 - get_f1: 0.7888 - val_loss: 0.7070 - val_get_f1: 0.7624\n",
      "Epoch 12/20\n",
      "25/25 [==============================] - 1s 32ms/step - loss: 0.6300 - get_f1: 0.7902 - val_loss: 0.7111 - val_get_f1: 0.7600\n"
     ]
    }
   ],
   "source": [
    "tensorboard=TensorBoard(log_dir='./logs', write_graph=True, write_images=True)\n",
    "early_stopping=EarlyStopping(monitor='val_loss')  \n",
    "epochs = 20\n",
    "batch_size = 512\n",
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_split=0.1,\n",
    "                    callbacks=[tensorboard, early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/14 [==============================] - 0s 9ms/step - loss: 0.6764 - get_f1: 0.7775\n",
      "\n",
      "\n",
      "Test loss: 0.6763707995414734\n",
      "Test f1_score: 0.7774753570556641\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(x_test, y_test, batch_size=batch_size, verbose=1)\n",
    "print('\\n')\n",
    "print('Test loss:', score[0])\n",
    "print('Test f1_score:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras CONV модель с предобученным Embedding слоем"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors = gensim.models.KeyedVectors.load_word2vec_format(os.path.join(r'/home/dmitriy/Downloads/ai_nlp_data/hw_7/', 'model.bin'), binary=True) .vectors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors_matrix = [word_vectors[i][:128] for i in range(200)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "initializer = tf.keras.initializers.Constant(word_vectors_matrix)\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=max_words, output_dim=128, embeddings_initializer =initializer, input_length=max_len))\n",
    "model.add(Conv1D(128, 3))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(GlobalMaxPool1D())\n",
    "model.add(Dense(10))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(Dense(num_classes))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=[get_f1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "25/25 [==============================] - 2s 48ms/step - loss: 0.9119 - get_f1: 0.7174 - val_loss: 0.8372 - val_get_f1: 0.7323\n",
      "Epoch 2/20\n",
      "25/25 [==============================] - 1s 34ms/step - loss: 0.7483 - get_f1: 0.7523 - val_loss: 0.7750 - val_get_f1: 0.7457\n",
      "Epoch 3/20\n",
      "25/25 [==============================] - 1s 34ms/step - loss: 0.6960 - get_f1: 0.7664 - val_loss: 0.7581 - val_get_f1: 0.7564\n",
      "Epoch 4/20\n",
      "25/25 [==============================] - 1s 33ms/step - loss: 0.6596 - get_f1: 0.7772 - val_loss: 0.7468 - val_get_f1: 0.7566\n",
      "Epoch 5/20\n",
      "25/25 [==============================] - 1s 33ms/step - loss: 0.6357 - get_f1: 0.7850 - val_loss: 0.7385 - val_get_f1: 0.7577\n",
      "Epoch 6/20\n",
      "25/25 [==============================] - 1s 33ms/step - loss: 0.6137 - get_f1: 0.7932 - val_loss: 0.7500 - val_get_f1: 0.7540\n"
     ]
    }
   ],
   "source": [
    "tensorboard=TensorBoard(log_dir='./logs', write_graph=True, write_images=True)\n",
    "early_stopping=EarlyStopping(monitor='val_loss')  \n",
    "\n",
    "epochs = 20\n",
    "batch_size = 512\n",
    "\n",
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_split=0.1,\n",
    "                    callbacks=[tensorboard, early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/14 [==============================] - 0s 9ms/step - loss: 0.7159 - get_f1: 0.7632\n",
      "\n",
      "\n",
      "Test loss: 0.7158985137939453\n",
      "Test f1_score: 0.7631778717041016\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(x_test, y_test, batch_size=batch_size, verbose=1)\n",
    "print('\\n')\n",
    "print('Test loss:', score[0])\n",
    "print('Test f1_score:', score[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
